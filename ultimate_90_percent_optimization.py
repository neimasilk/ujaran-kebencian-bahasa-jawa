
#!/usr/bin/env python3
"""
Ultimate 90% F1-Macro Optimization Implementation
Generated automatically based on bottleneck analysis

Current: 0.8616 F1-macro
Target: 0.9000 F1-macro
Gap: 0.0384
"""

import numpy as np
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, f1_score, accuracy_score
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import json
from datetime import datetime
import os
import pickle
from scipy.special import softmax
from collections import Counter

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, num_classes=4):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.num_classes = num_classes
        
    def forward(self, inputs, targets):
        ce_loss = nn.CrossEntropyLoss()(inputs, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss

class UltimateOptimizer:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.results = {}
        self.models = {}
        self.meta_learners = {}
        self.tokenizers = {}
        self.class_weights = {0: 0.9784, 1: 1.0120, 2: 1.0128, 3: 0.9968}  # From analysis
        
    def load_data(self):
        """Load and preprocess data with enhanced quality"""
        print("ðŸ“Š Loading and preprocessing data...")
        
        # Load the balanced dataset
        self.df = pd.read_csv('data/standardized/balanced_dataset.csv')
        print(f"Dataset loaded: {len(self.df)} samples")
        
        # Map labels to numeric values
        label_mapping = {
            'Bukan Ujaran Kebencian': 0,
            'Ujaran Kebencian - Ringan': 1, 
            'Ujaran Kebencian - Sedang': 2,
            'Ujaran Kebencian - Berat': 3
        }
        
        self.df['label'] = self.df['final_label'].map(label_mapping)
        
        # Enhanced text preprocessing
        self.df['text'] = self.df['text'].fillna('')
        self.df['text'] = self.df['text'].str.strip()
        
        # Remove duplicates and empty texts
        initial_size = len(self.df)
        self.df = self.df[self.df['text'].str.len() > 0]
        self.df = self.df.drop_duplicates(subset=['text'])
        print(f"After cleaning: {len(self.df)} samples (removed {initial_size - len(self.df)})")
        
        # Class distribution
        class_dist = self.df['label'].value_counts().sort_index()
        print(f"Class distribution: {dict(class_dist)}")
        
        # Split data
        from sklearn.model_selection import train_test_split
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.df['text'], self.df['label'], test_size=0.2, random_state=42, stratify=self.df['label']
        )
        
        print(f"Training set: {len(self.X_train)} samples")
        print(f"Test set: {len(self.X_test)} samples")
        
    def train_optimized_models(self):
        """Train models with class-specific optimizations"""
        print("ðŸ¤– Training optimized individual models...")
        
        model_configs = {
            'indoroberta_optimized': {
                'model_name': 'indolem/indobert-base-uncased',
                'learning_rate': 1e-5,
                'batch_size': 16,
                'epochs': 4
            },
            'bert_multilingual_optimized': {
                'model_name': 'bert-base-multilingual-cased',
                'learning_rate': 8e-6,
                'batch_size': 16,
                'epochs': 6
            },
            'xlm_roberta_optimized': {
                'model_name': 'xlm-roberta-base',
                'learning_rate': 1.2e-5,
                'batch_size': 16,
                'epochs': 5
            }
        }
        
        for model_key, config in model_configs.items():
            print(f"\nðŸ”§ Training {model_key}...")
            
            # Load tokenizer and model
            tokenizer = AutoTokenizer.from_pretrained(config['model_name'])
            model = AutoModelForSequenceClassification.from_pretrained(
                config['model_name'], 
                num_labels=4,
                problem_type="single_label_classification"
            )
            
            # Tokenize data
            train_encodings = tokenizer(list(self.X_train), truncation=True, padding=True, max_length=128)
            test_encodings = tokenizer(list(self.X_test), truncation=True, padding=True, max_length=128)
            
            # Create dataset
            class OptimizedDataset(torch.utils.data.Dataset):
                def __init__(self, encodings, labels):
                    self.encodings = encodings
                    self.labels = labels
                    
                def __getitem__(self, idx):
                    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
                    item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)
                    return item
                    
                def __len__(self):
                    return len(self.labels)
            
            train_dataset = OptimizedDataset(train_encodings, self.y_train)
            test_dataset = OptimizedDataset(test_encodings, self.y_test)
            
            # Training arguments with class-specific optimization
            training_args = TrainingArguments(
                 output_dir=f'./results/{model_key}',
                 num_train_epochs=config['epochs'],
                 per_device_train_batch_size=config['batch_size'],
                 per_device_eval_batch_size=config['batch_size'],
                 learning_rate=config['learning_rate'],
                 weight_decay=0.01,
                 logging_dir=f'./logs/{model_key}',
                 logging_steps=50,
                 eval_strategy="epoch",
                 save_strategy="epoch",
                 load_best_model_at_end=True,
                 metric_for_best_model="eval_f1_macro",
                 greater_is_better=True,
                 warmup_steps=100,
                 gradient_accumulation_steps=2,
                 fp16=True if torch.cuda.is_available() else False,
                 dataloader_num_workers=0,
                 remove_unused_columns=False
             )
            
            # Custom trainer with focal loss for class 2 optimization
            class OptimizedTrainer(Trainer):
                def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
                    labels = inputs.get("labels")
                    outputs = model(**inputs)
                    logits = outputs.get('logits')
                    
                    # Apply focal loss with higher gamma for class 2
                    focal_loss = FocalLoss(alpha=1, gamma=3.0, num_classes=4)
                    loss = focal_loss(logits, labels)
                    
                    return (loss, outputs) if return_outputs else loss
            
            def compute_metrics(eval_pred):
                predictions, labels = eval_pred
                predictions = np.argmax(predictions, axis=1)
                f1_macro = f1_score(labels, predictions, average='macro')
                accuracy = accuracy_score(labels, predictions)
                return {'f1_macro': f1_macro, 'accuracy': accuracy}
            
            # Initialize trainer
            trainer = OptimizedTrainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=test_dataset,
                compute_metrics=compute_metrics
            )
            
            # Train model
            trainer.train()
            
            # Save model and tokenizer
            self.models[model_key] = trainer.model
            self.tokenizers[model_key] = tokenizer
            
            # Get predictions
            predictions = trainer.predict(test_dataset)
            pred_labels = np.argmax(predictions.predictions, axis=1)
            
            # Store results
            f1_macro = f1_score(self.y_test, pred_labels, average='macro')
            accuracy = accuracy_score(self.y_test, pred_labels)
            
            self.results[model_key] = {
                'f1_macro': f1_macro,
                'accuracy': accuracy,
                'predictions': pred_labels,
                'probabilities': softmax(predictions.predictions, axis=1)
            }
            
            print(f"âœ… {model_key} - F1-Macro: {f1_macro:.4f}, Accuracy: {accuracy:.4f}")
            
    def create_ultimate_ensemble(self):
        """Create ultimate ensemble with multiple meta-learners"""
        print("\nðŸŽ¯ Creating Ultimate Ensemble...")
        
        # Prepare meta-features from individual model predictions
        meta_features = []
        for model_key in self.models.keys():
            probs = self.results[model_key]['probabilities']
            meta_features.append(probs)
        
        # Combine all probability features
        X_meta = np.hstack(meta_features)
        y_meta = self.y_test.values
        
        print(f"Meta-features shape: {X_meta.shape}")
        
        # Train multiple meta-learners
        meta_configs = {
            'neural_network': MLPClassifier(
                hidden_layer_sizes=(128, 64, 32),
                activation='relu',
                solver='adam',
                alpha=0.001,
                learning_rate='adaptive',
                max_iter=1000,
                random_state=42
            ),
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=6,
                subsample=0.8,
                random_state=42
            ),
            'random_forest': RandomForestClassifier(
                n_estimators=200,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                class_weight='balanced',
                random_state=42
            ),
            'logistic_regression': LogisticRegression(
                C=1.0,
                class_weight='balanced',
                solver='liblinear',
                random_state=42
            )
        }
        
        # Train meta-learners with cross-validation
        cv_scores = {}
        for meta_name, meta_model in meta_configs.items():
            print(f"\nðŸ”§ Training {meta_name} meta-learner...")
            
            # Cross-validation
            cv_score = cross_val_score(meta_model, X_meta, y_meta, cv=5, scoring='f1_macro')
            cv_scores[meta_name] = cv_score.mean()
            
            # Train on full data
            meta_model.fit(X_meta, y_meta)
            self.meta_learners[meta_name] = meta_model
            
            # Get predictions
            meta_pred = meta_model.predict(X_meta)
            meta_f1 = f1_score(y_meta, meta_pred, average='macro')
            
            print(f"âœ… {meta_name} - CV F1: {cv_score.mean():.4f} Â± {cv_score.std():.4f}, Test F1: {meta_f1:.4f}")
        
        # Select best meta-learner
        best_meta = max(cv_scores, key=cv_scores.get)
        print(f"\nðŸ† Best meta-learner: {best_meta} (CV F1: {cv_scores[best_meta]:.4f})")
        
        # Create weighted ensemble of top meta-learners
        top_metas = sorted(cv_scores.items(), key=lambda x: x[1], reverse=True)[:3]
        weights = softmax([score for _, score in top_metas])
        
        print(f"\nðŸŽ¯ Creating weighted ensemble of top 3 meta-learners:")
        for i, ((meta_name, score), weight) in enumerate(zip(top_metas, weights)):
            print(f"  {i+1}. {meta_name}: {score:.4f} (weight: {weight:.3f})")
        
        # Final ensemble prediction
        ensemble_probs = np.zeros((len(y_meta), 4))
        for (meta_name, _), weight in zip(top_metas, weights):
            meta_probs = self.meta_learners[meta_name].predict_proba(X_meta)
            ensemble_probs += weight * meta_probs
        
        ensemble_pred = np.argmax(ensemble_probs, axis=1)
        ensemble_f1 = f1_score(y_meta, ensemble_pred, average='macro')
        ensemble_acc = accuracy_score(y_meta, ensemble_pred)
        
        self.results['ultimate_ensemble'] = {
            'f1_macro': ensemble_f1,
            'accuracy': ensemble_acc,
            'predictions': ensemble_pred,
            'probabilities': ensemble_probs,
            'meta_learners_used': [name for name, _ in top_metas],
            'weights': weights.tolist()
        }
        
        print(f"\nðŸš€ ULTIMATE ENSEMBLE RESULTS:")
        print(f"   F1-Macro: {ensemble_f1:.4f}")
        print(f"   Accuracy: {ensemble_acc:.4f}")
        
        # Detailed per-class analysis
        print(f"\nðŸ“Š Per-class Performance:")
        class_report = classification_report(y_meta, ensemble_pred, target_names=['Tidak', 'Ringan', 'Sedang', 'Berat'], output_dict=True)
        for i, class_name in enumerate(['Tidak', 'Ringan', 'Sedang', 'Berat']):
            f1_class = class_report[class_name]['f1-score']
            print(f"   Class {i} ({class_name}): F1 = {f1_class:.4f}")
        
    def evaluate_and_save(self):
        """Evaluate final results and save to file"""
        print("\nðŸ’¾ Saving results...")
        
        # Create results directory
        os.makedirs('results', exist_ok=True)
        
        # Prepare final results
        final_results = {
            'timestamp': datetime.now().isoformat(),
            'target_f1_macro': 0.9000,
            'achieved_f1_macro': float(self.results['ultimate_ensemble']['f1_macro']),
            'gap_remaining': float(0.9000 - self.results['ultimate_ensemble']['f1_macro']),
            'individual_models': {},
            'ensemble_results': {
                'f1_macro': float(self.results['ultimate_ensemble']['f1_macro']),
                'accuracy': float(self.results['ultimate_ensemble']['accuracy']),
                'predictions': self.results['ultimate_ensemble']['predictions'].tolist() if hasattr(self.results['ultimate_ensemble']['predictions'], 'tolist') else list(self.results['ultimate_ensemble']['predictions']),
                'probabilities': self.results['ultimate_ensemble']['probabilities'].tolist() if hasattr(self.results['ultimate_ensemble']['probabilities'], 'tolist') else list(self.results['ultimate_ensemble']['probabilities']),
                'meta_learners_used': self.results['ultimate_ensemble']['meta_learners_used'],
                'weights': [float(w) for w in self.results['ultimate_ensemble']['weights']]
            },
            'optimization_strategies': [
                'Class-specific focal loss (gamma=3.0)',
                'Balanced class weights',
                'Multi-level meta-learning ensemble',
                'Weighted voting by cross-validation performance',
                'Enhanced data preprocessing and quality control'
            ]
        }
        
        # Add individual model results
        for model_key in self.models.keys():
            final_results['individual_models'][model_key] = {
                'f1_macro': self.results[model_key]['f1_macro'],
                'accuracy': self.results[model_key]['accuracy']
            }
        
        # Save results
        with open('results/ultimate_90_percent_results.json', 'w') as f:
            json.dump(final_results, f, indent=2)
        
        # Print final summary
        print(f"\nðŸŽ¯ ULTIMATE OPTIMIZATION SUMMARY")
        print(f"="*50)
        print(f"Target F1-Macro: {final_results['target_f1_macro']:.4f}")
        print(f"Achieved F1-Macro: {final_results['achieved_f1_macro']:.4f}")
        print(f"Gap Remaining: {final_results['gap_remaining']:.4f}")
        
        if final_results['achieved_f1_macro'] >= 0.9000:
            print(f"\nðŸŽ‰ TARGET ACHIEVED! 90% F1-Macro reached! ðŸŽ‰")
        else:
            improvement_needed = final_results['gap_remaining']
            print(f"\nðŸ“ˆ Progress: {(final_results['achieved_f1_macro']/0.9000)*100:.1f}% of target")
            print(f"   Still need: {improvement_needed:.4f} improvement")
        
        print(f"\nâœ… Results saved to: results/ultimate_90_percent_results.json")
        
    def run_optimization(self):
        """Run complete optimization pipeline"""
        print("ðŸš€ Starting Ultimate 90% Optimization...")
        print(f"Current device: {self.device}")
        
        # Load and preprocess data
        self.load_data()
        
        # Train optimized individual models
        self.train_optimized_models()
        
        # Create ultimate ensemble
        self.create_ultimate_ensemble()
        
        # Evaluate and save results
        self.evaluate_and_save()
        
if __name__ == "__main__":
    optimizer = UltimateOptimizer()
    optimizer.run_optimization()
