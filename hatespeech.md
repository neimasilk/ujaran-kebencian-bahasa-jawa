# **Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection**

# **Bertie Vidgen***†***, Tristan Thrush***‡***, Zeerak Waseem**٨**, Douwe Kiela***‡*

*†*The Alan Turing Institute; ٨University of Sheffield; *‡*Facebook AI Research

[bvidgen@turing.ac.uk](mailto:bvidgen@turing.ac.uk)

# **Abstract**

We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more ro- bust hate detection models. We provide a new dataset of ∼40*,* 000 entries, generated and la- belled by trained annotators over four rounds of dynamic data creation. It includes ∼15*,* 000 challenging perturbations and each hateful en- try has fine-grained labels for the type and tar- get of hate. Hateful entries make up 54% of the dataset, which is substantially higher than comparable datasets. We show that model per- formance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also have better performance on HATECHECK, a suite of functional tests for online hate detec- tion. We provide the code, dataset and annota- tion guidelines for other researchers to use.

1. # **Introduction**

Accurate detection of online hate speech is impor- tant for ensuring that such content can be found and tackled scalably, minimizing the risk that harm will be inflicted on victims and making online spaces more accessible and safe. However, detecting on- line hate has proven remarkably difficult and con- cerns have been raised about the performance, ro- bustness, generalisability and fairness of even state- of-the-art models ([Waseem et al.](#zeerak-waseem,-james-thorne,-and-joachim-bingel.-2018.-bridging-the-gaps:-multi-task-learning-for-domain-transfer-of-hate-speech-detection.-in-jennifer-golbeck,-editor,-online-harassment,-pages-29–55.-springer-international-publishing,-cham.), [2018](#zeerak-waseem,-james-thorne,-and-joachim-bingel.-2018.-bridging-the-gaps:-multi-task-learning-for-domain-transfer-of-hate-speech-detection.-in-jennifer-golbeck,-editor,-online-harassment,-pages-29–55.-springer-international-publishing,-cham.); [Vidgen](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.) [et al.](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.), [2019a](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.); [Caselli et al.](#tommaso-caselli,-valerio-basile,-jelena-mitrovic´,-inga-kartoziya,-and-michael-granitzer.-2020.-i-feel-of--fended,-don’t-be-abusive!-implicit/explicit-mes--sages-in-offensive-and-abusive-language.-in-pro--ceedings-of-the-12th-conference-on-language-re--sources-and-evaluation-\(lrec\),-pages-6193–6202.), [2020](#tommaso-caselli,-valerio-basile,-jelena-mitrovic´,-inga-kartoziya,-and-michael-granitzer.-2020.-i-feel-of--fended,-don’t-be-abusive!-implicit/explicit-mes--sages-in-offensive-and-abusive-language.-in-pro--ceedings-of-the-12th-conference-on-language-re--sources-and-evaluation-\(lrec\),-pages-6193–6202.); [Mishra et al.](#pushkar-mishra,-helen-yannakoudakis,-and-eka--terina-shutova.-2019.-tackling-online-abuse:-a-survey-of-automated-abuse-detection-methods.-arxiv:1908.06024v2,-pages-1–17.), [2019](#pushkar-mishra,-helen-yannakoudakis,-and-eka--terina-shutova.-2019.-tackling-online-abuse:-a-survey-of-automated-abuse-detection-methods.-arxiv:1908.06024v2,-pages-1–17.); [Poletto et al.](#fabio-poletto,-valerio-basile,-manuela-sanguinetti,-cristina-bosco,-and-viviana-patti.-2020.-resources-and-benchmark-corpora-for-hate-speech-detection:-a-systematic-review.-language-resources-and-evalu--ation,-pages-1–47.), [2020](#fabio-poletto,-valerio-basile,-manuela-sanguinetti,-cristina-bosco,-and-viviana-patti.-2020.-resources-and-benchmark-corpora-for-hate-speech-detection:-a-systematic-review.-language-resources-and-evalu--ation,-pages-1–47.)). To address these chal- lenges, we present a human-and-model-in-the-loop process for collecting data and training hate detec- tion models.

Our approach encompasses four rounds of data generation and model training. We first trained a classification model using previously released hate speech datasets. We then tasked annotators with

presenting content that would trick the model and yield *misclassifications*. At the end of the round we trained a new model using the newly presented data. In the next round the process was repeated with the new model in the loop for the annotators to trick. We had four rounds but this approach could, in principle, be continued indefinitely.

Round 1 contains original content created syn- thetically by annotators. Rounds 2, 3 and 4 are split into half original content and half pertur- bations. The perturbations are challenging ‘con- trast sets’, which manipulate the original text just enough to flip the label (e.g. from ‘Hate’ to ‘Not Hate’) ([Kaushik et al.](#divyansh-kaushik,-eduard-hovy,-and-zachary-c-lip--ton.-2019.-learning-the-difference-that-makes-a-difference-with-counterfactually-augmented-data.-arxiv-preprint-arxiv:1909.12434.), [2019](#divyansh-kaushik,-eduard-hovy,-and-zachary-c-lip--ton.-2019.-learning-the-difference-that-makes-a-difference-with-counterfactually-augmented-data.-arxiv-preprint-arxiv:1909.12434.); [Gardner et al.](#matt-gardner,-yoav-artzi,-victoria-basmova,-jonathan-berant,-ben-bogin,-sihao-chen,-pradeep-dasigi,-dheeru-dua,-yanai-elazar,-ananth-gottumukkala,-nitish-gupta,-hanna-hajishirzi,-gabriel-ilharco,-daniel-khashabi,-kevin-lin,-jiangming-liu,-nel--son-f.-liu,-phoebe-mulcaire,-qiang-ning,-sameer), [2020](#matt-gardner,-yoav-artzi,-victoria-basmova,-jonathan-berant,-ben-bogin,-sihao-chen,-pradeep-dasigi,-dheeru-dua,-yanai-elazar,-ananth-gottumukkala,-nitish-gupta,-hanna-hajishirzi,-gabriel-ilharco,-daniel-khashabi,-kevin-lin,-jiangming-liu,-nel--son-f.-liu,-phoebe-mulcaire,-qiang-ning,-sameer)). In Rounds 3 and 4 we also tasked annotators with exploring specific types of hate and taking close in- spiration from real-world hate sites to make content as adversarial, realistic, and varied as possible.

Models have lower accuracy when evaluated on test sets from later rounds as the content be- comes more adversarial. Similarly, the rate at which annotators trick models also decreases as rounds progress (see Table [3](#round)). At the same time, models trained on data from later rounds achieve higher accuracy, indicating that their performance improves (see Table [4](#model)). We verify improved model performance by evaluating them against the HATE\- CHECK functional tests ([Ro¨ttger et al.](#paul-ro¨ttger,-bertram-vidgen,-dong-nguyen,-zeerak-waseem,-helen-margetts,-and-janet-pierrehumbert.-2020.-hatecheck:-functional-tests-for-hate-speech-detection-models.), [2020](#paul-ro¨ttger,-bertram-vidgen,-dong-nguyen,-zeerak-waseem,-helen-margetts,-and-janet-pierrehumbert.-2020.-hatecheck:-functional-tests-for-hate-speech-detection-models.)), with accuracy improving from 60% in Round 1 to 95% in Round 4. In this way the models ‘learn from the worst’ because as the rounds progress (a) they be- come increasingly accurate in detecting hate which means that (b) annotators have to provide more challenging content in order to trick them.

We make three contributions to online hate clas- sification research. First, we present a human-and- model-in-the-loop process for training online hate detection models. Second, we present a dataset of 40*,* 000 entries, of which 54% are hate. It includes fine-grained annotations by trained annotators for

1667

*Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing*, pages 1667–1682  
August 1–6, 2021\. ©2021 Association for Computational Linguistics

label, type and target (where applicable). Third, we present high quality and robust hate detection models. All data, code and annotation guidelines are available.[1](#1https://github.com/bvidgen/)

2. # **Background** {#background}

**Benchmark datasets** Several benchmark datasets have been put forward for online hate classification ([Waseem and Hovy](#zeerak-waseem-and-dirk-hovy.-2016.-hateful-sym--bols-or-hateful-people?-predictive-features-for-hate-speech-detection-on-twitter.-in-naacl-hlt,-pages-88–93.), [2016](#zeerak-waseem-and-dirk-hovy.-2016.-hateful-sym--bols-or-hateful-people?-predictive-features-for-hate-speech-detection-on-twitter.-in-naacl-hlt,-pages-88–93.); [Waseem](#zeerak-waseem.-2016.-are-you-a-racist-or-am-i-seeing-things?-annotator-influence-on-hate-speech-detection-on-twitter.-in-proceedings-of-the-first-workshop-on-nlp-and-computational-social-science,-pages-138–-142,-austin,-texas.-association-for-computational-linguistics.), [2016](#zeerak-waseem.-2016.-are-you-a-racist-or-am-i-seeing-things?-annotator-influence-on-hate-speech-detection-on-twitter.-in-proceedings-of-the-first-workshop-on-nlp-and-computational-social-science,-pages-138–-142,-austin,-texas.-association-for-computational-linguistics.); [Davidson et al.](#thomas-davidson,-dana-warmsley,-michael-macy,-and-ingmar-weber.-2017.-automated-hate-speech-detection-and-the-problem-of-offensive-language.-in-proceedings-of-the-11th-icwsm,-pages-1–4.), [2017](#thomas-davidson,-dana-warmsley,-michael-macy,-and-ingmar-weber.-2017.-automated-hate-speech-detection-and-the-problem-of-offensive-language.-in-proceedings-of-the-11th-icwsm,-pages-1–4.); [Founta et al.](#antigoni-maria-founta,-constantinos-djouvas,-de--spoina-chatzakou,-ilias-leontiadis,-jeremy-black--burn,-gianluca-stringhini,-athena-vakali,-michael-sirivianos,-and-nicolas-kourtellis.-2018.-large-scale-crowdsourcing-and-characterization-of-twit--ter-abusive-behavior.-in-icwsm,-pages-1–11.), [2018](#antigoni-maria-founta,-constantinos-djouvas,-de--spoina-chatzakou,-ilias-leontiadis,-jeremy-black--burn,-gianluca-stringhini,-athena-vakali,-michael-sirivianos,-and-nicolas-kourtellis.-2018.-large-scale-crowdsourcing-and-characterization-of-twit--ter-abusive-behavior.-in-icwsm,-pages-1–11.);

[Mandl et al.](#thomas-mandl,-sandip-modha,-prasenjit-majumder,-daksh-patel,-mohana-dave,-chintak-mandlia,-and-aditya-patel.-2019.-overview-of-the-hasoc-track-at-fire-2019:-hate-speech-and-offensive-content-identification-in-indo-european-languages.-in-fire-’19:-proceedings-of-the-11th-forum-for-information-retrieval-evaluation,-pages-14–17.), [2019](#thomas-mandl,-sandip-modha,-prasenjit-majumder,-daksh-patel,-mohana-dave,-chintak-mandlia,-and-aditya-patel.-2019.-overview-of-the-hasoc-track-at-fire-2019:-hate-speech-and-offensive-content-identification-in-indo-european-languages.-in-fire-’19:-proceedings-of-the-11th-forum-for-information-retrieval-evaluation,-pages-14–17.); [Zampieri et al.](#marcos-zampieri,-shervin-malmasi,-preslav-nakov,-sara-rosenthal,-noura-farra,-and-ritesh-kumar.-2019.-predicting-the-type-and-target-of-offensive-posts-in-social-media.-in-proceedings-of-naacl-hlt-2019,-volume-1,-pages-1415–1420.), [2019](#marcos-zampieri,-shervin-malmasi,-preslav-nakov,-sara-rosenthal,-noura-farra,-and-ritesh-kumar.-2019.-predicting-the-type-and-target-of-offensive-posts-in-social-media.-in-proceedings-of-naacl-hlt-2019,-volume-1,-pages-1415–1420.), [2020](#marcos-zampieri,-preslav-nakov,-sara-rosenthal,-pepa-atanasova,-georgi-karadzhov,-hamdy-mubarak,-leon-derczynski,-zeses-pitenis,-and-c¸-ag˘rı-c¸-o¨ltekin.-2020.-semeval-2020-task-12:-multilingual-offen--sive-language-identification-in-social-media-\(offen--seval-2020\).-arxiv-preprint,-pages-1–23.); [Vidgen et al.](#bertie-vidgen,-austin-botelho,-david-broniatowski,-ella-guest,-matthew-hall,-helen-margetts,-rebekah-tromble,-zeerak-waseem,-and-scott-hale.-2020.-detecting-east-asian-prejudice-on-social-media.-arxiv:2005.03909v1,-pages-1–12.), [2020](#bertie-vidgen,-austin-botelho,-david-broniatowski,-ella-guest,-matthew-hall,-helen-margetts,-rebekah-tromble,-zeerak-waseem,-and-scott-hale.-2020.-detecting-east-asian-prejudice-on-social-media.-arxiv:2005.03909v1,-pages-1–12.), [2021](#bertie-vidgen,-dong-nguyen,-helen-margetts,-patricia-rossini,-and-rebekah-tromble.-2021.-introducing-cad:-the-contextual-abuse-dataset.-in-proceedings-of-the-2021-conference-of-the-north-american-chap--ter-of-the-association-for-computational-linguistics:-human-language-technologies,-pages-2289–2303,-online.-association-for-computational-linguistics.)). These datasets offer a comparison point for detection systems and have focused the field’s attention on important subtasks, such as classification across different languages, domains and targets of hate. Performance on some benchmark datasets has increased substantially through the use of more advanced models. For instance, in the original [Waseem and Hovy](#zeerak-waseem-and-dirk-hovy.-2016.-hateful-sym--bols-or-hateful-people?-predictive-features-for-hate-speech-detection-on-twitter.-in-naacl-hlt,-pages-88–93.) ([2016](#zeerak-waseem-and-dirk-hovy.-2016.-hateful-sym--bols-or-hateful-people?-predictive-features-for-hate-speech-detection-on-twitter.-in-naacl-hlt,-pages-88–93.)) paper in 2016, the authors achieved an F1 of 0*.*74. By 2018 this had increased to 0*.*93 ([Pitsilis et al.](#georgios-k.-pitsilis,-heri-ramampiaro,-and-helge-langseth.-2018.-detecting-offensive-language-in-tweets-using-deep-learning.-arxiv:1801.04433v1,-pages-1–17.), [2018](#georgios-k.-pitsilis,-heri-ramampiaro,-and-helge-langseth.-2018.-detecting-offensive-language-in-tweets-using-deep-learning.-arxiv:1801.04433v1,-pages-1–17.)).

Numerous problems have been identified with hate speech training datasets, such as lacking lin- guistic variety, being inexpertly annotated and de- grading over time ([Vidgen et al.](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.), [2019a](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.); [Poletto](#fabio-poletto,-valerio-basile,-manuela-sanguinetti,-cristina-bosco,-and-viviana-patti.-2020.-resources-and-benchmark-corpora-for-hate-speech-detection:-a-systematic-review.-language-resources-and-evalu--ation,-pages-1–47.) [et al.](#fabio-poletto,-valerio-basile,-manuela-sanguinetti,-cristina-bosco,-and-viviana-patti.-2020.-resources-and-benchmark-corpora-for-hate-speech-detection:-a-systematic-review.-language-resources-and-evalu--ation,-pages-1–47.), [2020](#fabio-poletto,-valerio-basile,-manuela-sanguinetti,-cristina-bosco,-and-viviana-patti.-2020.-resources-and-benchmark-corpora-for-hate-speech-detection:-a-systematic-review.-language-resources-and-evalu--ation,-pages-1–47.)). [Vidgen and Derczynski](#bertie-vidgen-and-leon-derczynski.-2020.-directions-in-abusive-language-training-data:-garbage-in,-garbage-out.-plos-one,-pages-1–26.) ([2020](#bertie-vidgen-and-leon-derczynski.-2020.-directions-in-abusive-language-training-data:-garbage-in,-garbage-out.-plos-one,-pages-1–26.)) ex- amined 63 open-source abusive language datasets and found that 27 (43%) were sourced from Twitter ([Vidgen and Derczynski](#bertie-vidgen-and-leon-derczynski.-2020.-directions-in-abusive-language-training-data:-garbage-in,-garbage-out.-plos-one,-pages-1–26.), [2020](#bertie-vidgen-and-leon-derczynski.-2020.-directions-in-abusive-language-training-data:-garbage-in,-garbage-out.-plos-one,-pages-1–26.)). In addition, many datasets are formed with bootstrapped sampling, such as keyword searches, due to the low preva- lence of hate speech ‘in the wild’ ([Vidgen et al.](#bertie-vidgen,-helen-margetts,-and-alex-harris.-2019b.-how-much-online-abuse-is-there?-a-systematic-re--view-of-evidence-for-the-uk.-the-alan-turing-insti--tute,-london.), [2019b](#bertie-vidgen,-helen-margetts,-and-alex-harris.-2019b.-how-much-online-abuse-is-there?-a-systematic-re--view-of-evidence-for-the-uk.-the-alan-turing-insti--tute,-london.)). Such bootstrapping can substantially bias the nature and coverage of datasets ([Wiegand et al.](#michael-wiegand,-josef-ruppenhofer,-and-thomas-kleinbauer.-2019.-detection-of-abusive-language:-the-problem-of-biased-datasets.-in-naacl-hlt,-pages-602–608,-minneapolis.-acl.), [2019](#michael-wiegand,-josef-ruppenhofer,-and-thomas-kleinbauer.-2019.-detection-of-abusive-language:-the-problem-of-biased-datasets.-in-naacl-hlt,-pages-602–608,-minneapolis.-acl.)). Models trained on historical data may also not be effective for present-day hate classification models given how quickly online conversations evolve ([Nobata et al.](#chikashi-nobata,-achint-thomas,-yashar-mehdad,-yi-chang,-and-joel-tetreault.-2016.-abusive-lan--guage-detection-in-online-user-content.-in-world-wide-web-conference,-pages-145–153.), [2016](#chikashi-nobata,-achint-thomas,-yashar-mehdad,-yi-chang,-and-joel-tetreault.-2016.-abusive-lan--guage-detection-in-online-user-content.-in-world-wide-web-conference,-pages-145–153.)).

**Model limitations** Systems trained on existing datasets have been shown to lack accuracy, robust- ness and generalisability, creating a range of false positives and false negatives ([Schmidt and Wie-](#anna-schmidt-and-michael-wiegand.-2017.-a-sur--vey-on-hate-speech-detection-using-natural-lan--guage-processing.-in-proceedings-of-the-fifth-inter--national-workshop-on-natural-language-processing-for-social-media,-pages-1–10.-association-for-com--putational-linguistics.) [gand](#anna-schmidt-and-michael-wiegand.-2017.-a-sur--vey-on-hate-speech-detection-using-natural-lan--guage-processing.-in-proceedings-of-the-fifth-inter--national-workshop-on-natural-language-processing-for-social-media,-pages-1–10.-association-for-com--putational-linguistics.), [2017](#anna-schmidt-and-michael-wiegand.-2017.-a-sur--vey-on-hate-speech-detection-using-natural-lan--guage-processing.-in-proceedings-of-the-fifth-inter--national-workshop-on-natural-language-processing-for-social-media,-pages-1–10.-association-for-com--putational-linguistics.); [Mishra et al.](#pushkar-mishra,-helen-yannakoudakis,-and-eka--terina-shutova.-2019.-tackling-online-abuse:-a-survey-of-automated-abuse-detection-methods.-arxiv:1908.06024v2,-pages-1–17.), [2019](#pushkar-mishra,-helen-yannakoudakis,-and-eka--terina-shutova.-2019.-tackling-online-abuse:-a-survey-of-automated-abuse-detection-methods.-arxiv:1908.06024v2,-pages-1–17.); [Vidgen and Der-](#bertie-vidgen-and-leon-derczynski.-2020.-directions-in-abusive-language-training-data:-garbage-in,-garbage-out.-plos-one,-pages-1–26.) [czynski](#bertie-vidgen-and-leon-derczynski.-2020.-directions-in-abusive-language-training-data:-garbage-in,-garbage-out.-plos-one,-pages-1–26.), [2020](#bertie-vidgen-and-leon-derczynski.-2020.-directions-in-abusive-language-training-data:-garbage-in,-garbage-out.-plos-one,-pages-1–26.); [Ro¨ttger et al.](#paul-ro¨ttger,-bertram-vidgen,-dong-nguyen,-zeerak-waseem,-helen-margetts,-and-janet-pierrehumbert.-2020.-hatecheck:-functional-tests-for-hate-speech-detection-models.), [2020](#paul-ro¨ttger,-bertram-vidgen,-dong-nguyen,-zeerak-waseem,-helen-margetts,-and-janet-pierrehumbert.-2020.-hatecheck:-functional-tests-for-hate-speech-detection-models.); [Mathew et al.](#binny-mathew,-punyajoy-saha,-seid-muhie-yi--mam,-chris-biemann,-pawan-goyal,-and-animesh-mukherjee.-2020.-hatexplain:-a-benchmark-dataset-for-explainable-hate-speech-detection.), [2020](#binny-mathew,-punyajoy-saha,-seid-muhie-yi--mam,-chris-biemann,-pawan-goyal,-and-animesh-mukherjee.-2020.-hatexplain:-a-benchmark-dataset-for-explainable-hate-speech-detection.)). These errors often make models unsuitable for use in downstream tasks, such as moderating online content or measuring online hate.

1[https://github.com/bvidgen/](https://github.com/bvidgen/Dynamically-Generated-Hate-Speech-Dataset)  
[Dynamically-Generated-Hate-Speech-Dataset](https://github.com/bvidgen/Dynamically-Generated-Hate-Speech-Dataset)

False positives are non-hateful entries which are incorrectly classified as hateful. [Vidgen et al.](#bertie-vidgen,-austin-botelho,-david-broniatowski,-ella-guest,-matthew-hall,-helen-margetts,-rebekah-tromble,-zeerak-waseem,-and-scott-hale.-2020.-detecting-east-asian-prejudice-on-social-media.-arxiv:2005.03909v1,-pages-1–12.) ([2020](#bertie-vidgen,-austin-botelho,-david-broniatowski,-ella-guest,-matthew-hall,-helen-margetts,-rebekah-tromble,-zeerak-waseem,-and-scott-hale.-2020.-detecting-east-asian-prejudice-on-social-media.-arxiv:2005.03909v1,-pages-1–12.)) report that 29% of errors from a classifier for East Asian prejudice are due to lexical similari- ties between hateful and non-hateful entries, such as abuse directed towards out-of-scope targets be- ing misclassified as Sinophobic. Other research shows that some identity terms (e.g. ‘gay’) are substantially more likely to appear in toxic content in training datasets, leading models to overfit on them ([Dixon et al.](#lucas-dixon,-john-li,-jeffrey-sorensen,-nithum-thain,-and-lucy-vasserman.-2018.-measuring-and-miti--gating-unintended-bias-in-text-classification.-in-aaai/acm-conference-on-ai,-ethics,-and-society,-pages-67–73.), [2018](#lucas-dixon,-john-li,-jeffrey-sorensen,-nithum-thain,-and-lucy-vasserman.-2018.-measuring-and-miti--gating-unintended-bias-in-text-classification.-in-aaai/acm-conference-on-ai,-ethics,-and-society,-pages-67–73.); [Kennedy et al.](#brendan-kennedy,-xisen-jin,-aida-mostafazadeh-da--vani,-morteza-dehghani,-and-xiang-ren.-2020.-con--textualizing-hate-speech-classifiers-with-post-hoc-explanation.-in-proceedings-of-the-58th-annual-meeting-of-the-association-for-computational-lin--guistics,-pages-5435–5442.), [2020](#brendan-kennedy,-xisen-jin,-aida-mostafazadeh-da--vani,-morteza-dehghani,-and-xiang-ren.-2020.-con--textualizing-hate-speech-classifiers-with-post-hoc-explanation.-in-proceedings-of-the-58th-annual-meeting-of-the-association-for-computational-lin--guistics,-pages-5435–5442.)). Similarly, many models overfit on the use of slurs and pejorative terms, treating them as hateful irre- spective of how they are used ([Waseem et al.](#zeerak-waseem,-james-thorne,-and-joachim-bingel.-2018.-bridging-the-gaps:-multi-task-learning-for-domain-transfer-of-hate-speech-detection.-in-jennifer-golbeck,-editor,-online-harassment,-pages-29–55.-springer-international-publishing,-cham.), [2018](#zeerak-waseem,-james-thorne,-and-joachim-bingel.-2018.-bridging-the-gaps:-multi-task-learning-for-domain-transfer-of-hate-speech-detection.-in-jennifer-golbeck,-editor,-online-harassment,-pages-29–55.-springer-international-publishing,-cham.); [Davidson et al.](#thomas-davidson,-dana-warmsley,-michael-macy,-and-ingmar-weber.-2017.-automated-hate-speech-detection-and-the-problem-of-offensive-language.-in-proceedings-of-the-11th-icwsm,-pages-1–4.), [2017](#thomas-davidson,-dana-warmsley,-michael-macy,-and-ingmar-weber.-2017.-automated-hate-speech-detection-and-the-problem-of-offensive-language.-in-proceedings-of-the-11th-icwsm,-pages-1–4.); [Kurrek et al.](#jana-kurrek,-haji-mohammad-saleem,-and-derek-ruths.-2020.-towards-a-comprehensive-taxonomy-and-large-scale-annotated-corpus-for-online-slur-usage.-in-proceedings-of-the-fourth-workshop-on-online-abuse-and-harms,-pages-138–149.), [2020](#jana-kurrek,-haji-mohammad-saleem,-and-derek-ruths.-2020.-towards-a-comprehensive-taxonomy-and-large-scale-annotated-corpus-for-online-slur-usage.-in-proceedings-of-the-fourth-workshop-on-online-abuse-and-harms,-pages-138–149.); [Palmer](#alexis-palmer,-christine-carr,-melissa-robinson,-and-jordan-sanders.-2020.-cold:-annotation-scheme-and-evaluation-data-set-for-complex-offensive-lan--guage-in-english.-the-journal-for-language-tech--nology-and-computational-linguistics,-34\(1\):1–28.) [et al.](#alexis-palmer,-christine-carr,-melissa-robinson,-and-jordan-sanders.-2020.-cold:-annotation-scheme-and-evaluation-data-set-for-complex-offensive-lan--guage-in-english.-the-journal-for-language-tech--nology-and-computational-linguistics,-34\(1\):1–28.), [2020](#alexis-palmer,-christine-carr,-melissa-robinson,-and-jordan-sanders.-2020.-cold:-annotation-scheme-and-evaluation-data-set-for-complex-offensive-lan--guage-in-english.-the-journal-for-language-tech--nology-and-computational-linguistics,-34\(1\):1–28.)). This is problematic when the terms are used as part of counter speech ([Wright et al.](#lucas-wright,-derek-ruths,-kelly-p-dillon,-haji-mo--hammad-saleem,-and-susan-benesch.-2017.-vec--tors-for-counterspeech-on-twitter.-in-proceedings-of-the-first-workshop-on-abusive-language-online,-pages-57–62,-vancouver,-bc,-canada.-association-for-computational-linguistics.), [2017](#lucas-wright,-derek-ruths,-kelly-p-dillon,-haji-mo--hammad-saleem,-and-susan-benesch.-2017.-vec--tors-for-counterspeech-on-twitter.-in-proceedings-of-the-first-workshop-on-abusive-language-online,-pages-57–62,-vancouver,-bc,-canada.-association-for-computational-linguistics.); [Chung et al.](#yi-ling-chung,-elizaveta-kuzmenko,-serra-sinem-tekiroglu,-and-marco-guerini.-2019.-conan---counter-narratives-through-nichesourcing:-a-mul--tilingual-dataset-of-responses-to-fight-online-hate-speech.-in-proceedings-of-the-57th-annual-meeting-of-the-acl,-pages-2819–2829.), [2019](#yi-ling-chung,-elizaveta-kuzmenko,-serra-sinem-tekiroglu,-and-marco-guerini.-2019.-conan---counter-narratives-through-nichesourcing:-a-mul--tilingual-dataset-of-responses-to-fight-online-hate-speech.-in-proceedings-of-the-57th-annual-meeting-of-the-acl,-pages-2819–2829.)) or have been reclaimed by the targeted group ([Waseem et al.](#zeerak-waseem,-james-thorne,-and-joachim-bingel.-2018.-bridging-the-gaps:-multi-task-learning-for-domain-transfer-of-hate-speech-detection.-in-jennifer-golbeck,-editor,-online-harassment,-pages-29–55.-springer-international-publishing,-cham.), [2018](#zeerak-waseem,-james-thorne,-and-joachim-bingel.-2018.-bridging-the-gaps:-multi-task-learning-for-domain-transfer-of-hate-speech-detection.-in-jennifer-golbeck,-editor,-online-harassment,-pages-29–55.-springer-international-publishing,-cham.); [Sap](#maarten-sap,-dallas-card,-saadia-gabriel,-yejin-choi,-noah-a-smith,-and-paul-g-allen.-2019.-the-risk-of-racial-bias-in-hate-speech-detection.-in-proceed--ings-of-the-57th-annual-meeting-of-the-acl,-pages-1668–1678.) [et al.](#maarten-sap,-dallas-card,-saadia-gabriel,-yejin-choi,-noah-a-smith,-and-paul-g-allen.-2019.-the-risk-of-racial-bias-in-hate-speech-detection.-in-proceed--ings-of-the-57th-annual-meeting-of-the-acl,-pages-1668–1678.), [2019](#maarten-sap,-dallas-card,-saadia-gabriel,-yejin-choi,-noah-a-smith,-and-paul-g-allen.-2019.-the-risk-of-racial-bias-in-hate-speech-detection.-in-proceed--ings-of-the-57th-annual-meeting-of-the-acl,-pages-1668–1678.)). Models can also misclassify interper- sonal abuse and incivil language as hateful ([Wul-](#ellery-wulczyn,-nithum-thain,-and-lucas-dixon.-2017a.-ex-machina:-personal-attacks-seen-at-scale.-in-proceedings-of-the-international-world-wide-web-conference,-pages-1391–1399.) [czyn et al.](#ellery-wulczyn,-nithum-thain,-and-lucas-dixon.-2017a.-ex-machina:-personal-attacks-seen-at-scale.-in-proceedings-of-the-international-world-wide-web-conference,-pages-1391–1399.), [2017a](#ellery-wulczyn,-nithum-thain,-and-lucas-dixon.-2017a.-ex-machina:-personal-attacks-seen-at-scale.-in-proceedings-of-the-international-world-wide-web-conference,-pages-1391–1399.); [Zampieri et al.](#marcos-zampieri,-shervin-malmasi,-preslav-nakov,-sara-rosenthal,-noura-farra,-and-ritesh-kumar.-2019.-predicting-the-type-and-target-of-offensive-posts-in-social-media.-in-proceedings-of-naacl-hlt-2019,-volume-1,-pages-1415–1420.), [2019](#marcos-zampieri,-shervin-malmasi,-preslav-nakov,-sara-rosenthal,-noura-farra,-and-ritesh-kumar.-2019.-predicting-the-type-and-target-of-offensive-posts-in-social-media.-in-proceedings-of-naacl-hlt-2019,-volume-1,-pages-1415–1420.); [Palmer](#alexis-palmer,-christine-carr,-melissa-robinson,-and-jordan-sanders.-2020.-cold:-annotation-scheme-and-evaluation-data-set-for-complex-offensive-lan--guage-in-english.-the-journal-for-language-tech--nology-and-computational-linguistics,-34\(1\):1–28.) [et al.](#alexis-palmer,-christine-carr,-melissa-robinson,-and-jordan-sanders.-2020.-cold:-annotation-scheme-and-evaluation-data-set-for-complex-offensive-lan--guage-in-english.-the-journal-for-language-tech--nology-and-computational-linguistics,-34\(1\):1–28.), [2020](#alexis-palmer,-christine-carr,-melissa-robinson,-and-jordan-sanders.-2020.-cold:-annotation-scheme-and-evaluation-data-set-for-complex-offensive-lan--guage-in-english.-the-journal-for-language-tech--nology-and-computational-linguistics,-34\(1\):1–28.)).

False negatives are hateful entries which are in- correctly classified as non-hateful. [Gro¨ndahl et al.](#tommi-gro¨ndahl,-luca-pajola,-mika-juuti,-mauro-conti,-and-n.-asokan.-2018.-all-you-need-is-”love”:-evading-hate-speech-detection.-in-pro--ceedings-of-the-11th-acm-workshop-on-artificial-in--telligence-and-security,-pages-2–12.) ([2018](#tommi-gro¨ndahl,-luca-pajola,-mika-juuti,-mauro-conti,-and-n.-asokan.-2018.-all-you-need-is-”love”:-evading-hate-speech-detection.-in-pro--ceedings-of-the-11th-acm-workshop-on-artificial-in--telligence-and-security,-pages-2–12.)) show that making simple changes such as inserting spelling errors, using leetspeak[2](#2leetspeak-refers-to-the-obfuscation-of-words-by-replacing-letters-with-similar-looking-numbers-and-symbols.), chang- ing word boundaries, and appending words can lead to misclassifications of hate. [Hosseini et al.](#hossein-hosseini,-sreeram-kannan,-baosen-zhang,-and-radha-poovendran.-2017.-deceiving-google’s-perspective-api-built-for-detecting-toxic-com--ments.-in-proceedings-of-the-ieee-conference-on-computer-vision-and-pattern-recognition-\(cvpr\)-workshops,-pages-1305–1309.) ([2017](#hossein-hosseini,-sreeram-kannan,-baosen-zhang,-and-radha-poovendran.-2017.-deceiving-google’s-perspective-api-built-for-detecting-toxic-com--ments.-in-proceedings-of-the-ieee-conference-on-computer-vision-and-pattern-recognition-\(cvpr\)-workshops,-pages-1305–1309.)) also investigate how detection models can be attacked and report similar findings. In other cases, false negatives can be provoked by changing the ‘sensitive’ attribute of hateful content, such as changing the target from ‘gay’ to ‘black’ people ([Garg et al.](#sahaj-garg,-ankur-taly,-vincent-perot,-ed-h.-chi,-nicole-limtiaco,-and-alex-beutel.-2019.-counter--factual-fairness-in-text-classification-through-robust--ness.-proceedings-of-the-2019-aaai/acm-confer--ence-on-ai,-ethics,-and-society,-pages-219–226.), [2019](#sahaj-garg,-ankur-taly,-vincent-perot,-ed-h.-chi,-nicole-limtiaco,-and-alex-beutel.-2019.-counter--factual-fairness-in-text-classification-through-robust--ness.-proceedings-of-the-2019-aaai/acm-confer--ence-on-ai,-ethics,-and-society,-pages-219–226.)). This can happen when mod- els are trained on data which only contains hate directed against a limited set of targets ([Salminen](#joni-salminen,-maximilian-hopf,-shammur-a.-chowd--hury,-soon-gyo-jung,-hind-almerekhi,-and-bernard-j.-jansen.-2020.-developing-an-on--line-hate-classifier-for-multiple-social-media-plat--forms.-human-centric-computing-and-information-sciences,-10\(1\):1–34.) [et al.](#joni-salminen,-maximilian-hopf,-shammur-a.-chowd--hury,-soon-gyo-jung,-hind-almerekhi,-and-bernard-j.-jansen.-2020.-developing-an-on--line-hate-classifier-for-multiple-social-media-plat--forms.-human-centric-computing-and-information-sciences,-10\(1\):1–34.), [2020](#joni-salminen,-maximilian-hopf,-shammur-a.-chowd--hury,-soon-gyo-jung,-hind-almerekhi,-and-bernard-j.-jansen.-2020.-developing-an-on--line-hate-classifier-for-multiple-social-media-plat--forms.-human-centric-computing-and-information-sciences,-10\(1\):1–34.)). Another source of false negatives is when classification systems are applied to out-of- domain settings, such as system trained on Twitter data being applied to data from Gab ([Karan and](#mladen-karan-and-jan-sˇnajder.-2018.-cross-domain-detection-of-abusive-language-online.-in-2nd-workshop-on-abusive-language-online,-pages-132–-137.) [Sˇnajder](#mladen-karan-and-jan-sˇnajder.-2018.-cross-domain-detection-of-abusive-language-online.-in-2nd-workshop-on-abusive-language-online,-pages-132–-137.), [2018](#mladen-karan-and-jan-sˇnajder.-2018.-cross-domain-detection-of-abusive-language-online.-in-2nd-workshop-on-abusive-language-online,-pages-132–-137.); [Pamungkas et al.](#endang-pamungkas,-valerio-basile,-and-viviana-patti.-2020.-misogyny-detection-in-twitter:-a-multilin--gual-and-cross-domain-study.-information-process--ing-and-management,-57\(6\).), [2020](#endang-pamungkas,-valerio-basile,-and-viviana-patti.-2020.-misogyny-detection-in-twitter:-a-multilin--gual-and-cross-domain-study.-information-process--ing-and-management,-57\(6\).); [Swamy](#steve-durairaj-swamy,-anupam-jamatia,-and-bjo¨rn-gamba¨ck.-2019.-studying-generalisability-across-abusive-language-detection-datasets.-in-conll-2019---23rd-conference-on-computational-natural-language-learning,-proceedings-of-the-conference,-pages-940–950.) [et al.](#steve-durairaj-swamy,-anupam-jamatia,-and-bjo¨rn-gamba¨ck.-2019.-studying-generalisability-across-abusive-language-detection-datasets.-in-conll-2019---23rd-conference-on-computational-natural-language-learning,-proceedings-of-the-conference,-pages-940–950.), [2019](#steve-durairaj-swamy,-anupam-jamatia,-and-bjo¨rn-gamba¨ck.-2019.-studying-generalisability-across-abusive-language-detection-datasets.-in-conll-2019---23rd-conference-on-computational-natural-language-learning,-proceedings-of-the-conference,-pages-940–950.); [Basile et al.](#valerio-basile,-cristina-bosco,-elisabetta-fersini,-deb--ora-nozza,-viviana-patti,-francisco-manuel-rangel-pardo,-paolo-rosso,-and-manuela-sanguinetti.-2019.-sem-eval-2019-task-5:-multilingual-detection-of-hate-speech-against-immigrants-and-women-in-twitter.-in-proceedings-of-the-13th-international-workshop-on-semantic-evaluation,-pages-54–63.), [2019](#valerio-basile,-cristina-bosco,-elisabetta-fersini,-deb--ora-nozza,-viviana-patti,-francisco-manuel-rangel-pardo,-paolo-rosso,-and-manuela-sanguinetti.-2019.-sem-eval-2019-task-5:-multilingual-detection-of-hate-speech-against-immigrants-and-women-in-twitter.-in-proceedings-of-the-13th-international-workshop-on-semantic-evaluation,-pages-54–63.); [Salminen et al.](#joni-salminen,-maximilian-hopf,-shammur-a.-chowd--hury,-soon-gyo-jung,-hind-almerekhi,-and-bernard-j.-jansen.-2020.-developing-an-on--line-hate-classifier-for-multiple-social-media-plat--forms.-human-centric-computing-and-information-sciences,-10\(1\):1–34.), [2020](#joni-salminen,-maximilian-hopf,-shammur-a.-chowd--hury,-soon-gyo-jung,-hind-almerekhi,-and-bernard-j.-jansen.-2020.-developing-an-on--line-hate-classifier-for-multiple-social-media-plat--forms.-human-centric-computing-and-information-sciences,-10\(1\):1–34.)). Subtle and implicit forms of hate speech can also create false negatives ([Vidgen and Yasseri](#bertie-vidgen-and-taha-yasseri.-2019.-detecting-weak-and-strong-islamophobic-hate-speech-on-social-me--dia.-journal-of-information-technology-&-politics,-17\(1\):66–78.), [2019](#bertie-vidgen-and-taha-yasseri.-2019.-detecting-weak-and-strong-islamophobic-hate-speech-on-social-me--dia.-journal-of-information-technology-&-politics,-17\(1\):66–78.); [Palmer et al.](#alexis-palmer,-christine-carr,-melissa-robinson,-and-jordan-sanders.-2020.-cold:-annotation-scheme-and-evaluation-data-set-for-complex-offensive-lan--guage-in-english.-the-journal-for-language-tech--nology-and-computational-linguistics,-34\(1\):1–28.), [2020](#alexis-palmer,-christine-carr,-melissa-robinson,-and-jordan-sanders.-2020.-cold:-annotation-scheme-and-evaluation-data-set-for-complex-offensive-lan--guage-in-english.-the-journal-for-language-tech--nology-and-computational-linguistics,-34\(1\):1–28.); [Mathew et al.](#binny-mathew,-punyajoy-saha,-seid-muhie-yi--mam,-chris-biemann,-pawan-goyal,-and-animesh-mukherjee.-2020.-hatexplain:-a-benchmark-dataset-for-explainable-hate-speech-detection.), [2020](#binny-mathew,-punyajoy-saha,-seid-muhie-yi--mam,-chris-biemann,-pawan-goyal,-and-animesh-mukherjee.-2020.-hatexplain:-a-benchmark-dataset-for-explainable-hate-speech-detection.)), as well as more ‘complex’ forms of speech such as sarcasm, irony, adjective nominalization and rhetor- ical questions ([Caselli et al.](#tommaso-caselli,-valerio-basile,-jelena-mitrovic´,-inga-kartoziya,-and-michael-granitzer.-2020.-i-feel-of--fended,-don’t-be-abusive!-implicit/explicit-mes--sages-in-offensive-and-abusive-language.-in-pro--ceedings-of-the-12th-conference-on-language-re--sources-and-evaluation-\(lrec\),-pages-6193–6202.), [2020](#tommaso-caselli,-valerio-basile,-jelena-mitrovic´,-inga-kartoziya,-and-michael-granitzer.-2020.-i-feel-of--fended,-don’t-be-abusive!-implicit/explicit-mes--sages-in-offensive-and-abusive-language.-in-pro--ceedings-of-the-12th-conference-on-language-re--sources-and-evaluation-\(lrec\),-pages-6193–6202.); [Vidgen et al.](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.),

2Leetspeak refers to the obfuscation of words by replacing letters with similar looking numbers and symbols.

[2019a](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.)).

**Dynamic benchmarking and contrast sets** Ad- dressing the numerous flaws of hate detection mod- els is a difficult task. The problem may partly lie in the use of static benchmark datasets and fixed model evaluations. In other areas of Natu- ral Language Processing, several alternative model training and dataset construction paradigms have been presented, involving dynamic and iterative approaches. In a dynamic dataset creation setup, annotators are incentivised to produce high-quality ‘adversarial’ samples which are challenging for baseline models, repeating the process over mul- tiple rounds ([Nie et al.](#yixin-nie,-adina-williams,-emily-dinan,-mohit-bansal,-jason-weston,-and-douwe-kiela.-2020.-ad--versarial-nli:-a-new-benchmark-for-natural-lan--guage-understanding.-in-proceedings-of-the-58th-an--nual-meeting-of-the-association-for-computational-linguistics,-pages-4885–4901,-online.-association-for-computational-linguistics.), [2020](#yixin-nie,-adina-williams,-emily-dinan,-mohit-bansal,-jason-weston,-and-douwe-kiela.-2020.-ad--versarial-nli:-a-new-benchmark-for-natural-lan--guage-understanding.-in-proceedings-of-the-58th-an--nual-meeting-of-the-association-for-computational-linguistics,-pages-4885–4901,-online.-association-for-computational-linguistics.)). This offers a more targeted way of collecting data. [Dinan](#emily-dinan,-samuel-humeau,-bharath-chintagunta,-and-jason-weston.-2019.-build-it-break-it-fix-it-for-dialogue-safety:-robustness-from-adversarial-human-attack.-in-proceedings-of-the-2019-conference-on-empirical-methods-in-natural-language-processing,-pages-4537–4546.) [et al.](#emily-dinan,-samuel-humeau,-bharath-chintagunta,-and-jason-weston.-2019.-build-it-break-it-fix-it-for-dialogue-safety:-robustness-from-adversarial-human-attack.-in-proceedings-of-the-2019-conference-on-empirical-methods-in-natural-language-processing,-pages-4537–4546.) ([2019](#emily-dinan,-samuel-humeau,-bharath-chintagunta,-and-jason-weston.-2019.-build-it-break-it-fix-it-for-dialogue-safety:-robustness-from-adversarial-human-attack.-in-proceedings-of-the-2019-conference-on-empirical-methods-in-natural-language-processing,-pages-4537–4546.)) ask crowd-workers to ‘break’ a BERT model trained to identify toxic comments and then retrain it using the new examples. Their final model is more robust to complex forms of offensive con- tent, such as entries with figurative language and without profanities.

Another way of addressing the limitations of static datasets is through creating ‘contrast sets’ of perturbations ([Kaushik et al.](#divyansh-kaushik,-eduard-hovy,-and-zachary-c-lip--ton.-2019.-learning-the-difference-that-makes-a-difference-with-counterfactually-augmented-data.-arxiv-preprint-arxiv:1909.12434.), [2019](#divyansh-kaushik,-eduard-hovy,-and-zachary-c-lip--ton.-2019.-learning-the-difference-that-makes-a-difference-with-counterfactually-augmented-data.-arxiv-preprint-arxiv:1909.12434.); [Gardner et al.](#matt-gardner,-yoav-artzi,-victoria-basmova,-jonathan-berant,-ben-bogin,-sihao-chen,-pradeep-dasigi,-dheeru-dua,-yanai-elazar,-ananth-gottumukkala,-nitish-gupta,-hanna-hajishirzi,-gabriel-ilharco,-daniel-khashabi,-kevin-lin,-jiangming-liu,-nel--son-f.-liu,-phoebe-mulcaire,-qiang-ning,-sameer), [2020](#matt-gardner,-yoav-artzi,-victoria-basmova,-jonathan-berant,-ben-bogin,-sihao-chen,-pradeep-dasigi,-dheeru-dua,-yanai-elazar,-ananth-gottumukkala,-nitish-gupta,-hanna-hajishirzi,-gabriel-ilharco,-daniel-khashabi,-kevin-lin,-jiangming-liu,-nel--son-f.-liu,-phoebe-mulcaire,-qiang-ning,-sameer)). By making minimal label-changing mod- ifications that preserve ‘lexical/syntactic artifacts present in the original example’ ([Gardner et al.](#matt-gardner,-yoav-artzi,-victoria-basmova,-jonathan-berant,-ben-bogin,-sihao-chen,-pradeep-dasigi,-dheeru-dua,-yanai-elazar,-ananth-gottumukkala,-nitish-gupta,-hanna-hajishirzi,-gabriel-ilharco,-daniel-khashabi,-kevin-lin,-jiangming-liu,-nel--son-f.-liu,-phoebe-mulcaire,-qiang-ning,-sameer), [2020](#matt-gardner,-yoav-artzi,-victoria-basmova,-jonathan-berant,-ben-bogin,-sihao-chen,-pradeep-dasigi,-dheeru-dua,-yanai-elazar,-ananth-gottumukkala,-nitish-gupta,-hanna-hajishirzi,-gabriel-ilharco,-daniel-khashabi,-kevin-lin,-jiangming-liu,-nel--son-f.-liu,-phoebe-mulcaire,-qiang-ning,-sameer), p. 1308\) the risk of overfitting on spuri- ous correlations is minimized. Perturbations have only received limited attention in the context of hate detection. [Samory et al.](#mattia-samory,-indira-sen,-julian-kohne,-fabian-flock,-and-claudia-wagner.-2020.-unsex-me-here:-revisit--ing-sexism-detection-using-psychological-scales-and-adversarial-samples.-arxiv-preprint:2004.12764v1,-pages-1–11.) ([2020](#mattia-samory,-indira-sen,-julian-kohne,-fabian-flock,-and-claudia-wagner.-2020.-unsex-me-here:-revisit--ing-sexism-detection-using-psychological-scales-and-adversarial-samples.-arxiv-preprint:2004.12764v1,-pages-1–11.)) create 2*,* 000 ‘hard-to-classify’ not-sexist examples which con- trast sexist examples in their dataset. They show that fine-tuning a BERT model with the contrast set produces more robust classification system.

Dynamic benchmarking and contrast sets high- light the effectiveness of developing datasets in a directed and adaptive way, ensuring that models learn from and are evaluated on the most challeng- ing content. However, to date, these approaches remain under-explored for hate speech detection and to the best of our knowledge no prior work in hate speech detection has combined the two ap- proaches within one system.

3. # **Dataset labels**

Previous research shows the limitations of using only a binary labelling schema (i.e., ‘Hate’ and ‘Not Hate’). However, there are few established taxonomies and standards in online hate research, and most of the existing datasets have been labelled

with very different schemas. The hierarchical tax- onomy we present aims for a balance between gran- ularity versus conceptual distinctiveness and anno- tation simplicity, following the guidance of [Nicker-](#robert-c-nickerson,-upkar-varshney,-and-jan-munter--mann.-2013.-a-method-for-taxonomy-development-and-its-application-in-information-systems.-euro--pean-journal-of-information-systems,-22\(3\):336–-359.) [son et al.](#robert-c-nickerson,-upkar-varshney,-and-jan-munter--mann.-2013.-a-method-for-taxonomy-development-and-its-application-in-information-systems.-euro--pean-journal-of-information-systems,-22\(3\):336–-359.) ([2013](#robert-c-nickerson,-upkar-varshney,-and-jan-munter--mann.-2013.-a-method-for-taxonomy-development-and-its-application-in-information-systems.-euro--pean-journal-of-information-systems,-22\(3\):336–-359.)). All entries are assigned to either ‘Hate’ or ‘Not Hate’. ‘Hate’ is defined as “abu- sive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation.” ([Warner and Hirschberg](#william-warner-and-julia-hirschberg.-2012.-detecting-hate-speech-on-the-world-wide-web.-in-proceedings-of-the-second-workshop-on-language-in-social-me--dia,-pages-19–26.), [2012](#william-warner-and-julia-hirschberg.-2012.-detecting-hate-speech-on-the-world-wide-web.-in-proceedings-of-the-second-workshop-on-language-in-social-me--dia,-pages-19–26.)). For ‘Hate’, we also annotate secondary labels for the type and target of hate. The taxonomy for the type of hate draws on and extends previous work, in- cluding [Waseem and Hovy](#zeerak-waseem-and-dirk-hovy.-2016.-hateful-sym--bols-or-hateful-people?-predictive-features-for-hate-speech-detection-on-twitter.-in-naacl-hlt,-pages-88–93.) ([2016](#zeerak-waseem-and-dirk-hovy.-2016.-hateful-sym--bols-or-hateful-people?-predictive-features-for-hate-speech-detection-on-twitter.-in-naacl-hlt,-pages-88–93.)); [Vidgen et al.](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.) ([2019a](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.)); [Zampieri et al.](#marcos-zampieri,-shervin-malmasi,-preslav-nakov,-sara-rosenthal,-noura-farra,-and-ritesh-kumar.-2019.-predicting-the-type-and-target-of-offensive-posts-in-social-media.-in-proceedings-of-naacl-hlt-2019,-volume-1,-pages-1415–1420.) ([2019](#marcos-zampieri,-shervin-malmasi,-preslav-nakov,-sara-rosenthal,-noura-farra,-and-ritesh-kumar.-2019.-predicting-the-type-and-target-of-offensive-posts-in-social-media.-in-proceedings-of-naacl-hlt-2019,-volume-1,-pages-1415–1420.)).

1. ## **Types of hate**

**Derogation** Content which explicitly attacks, de- monizes, demeans or insults a group. This resem- bles similar definitions from [Davidson et al.](#thomas-davidson,-dana-warmsley,-michael-macy,-and-ingmar-weber.-2017.-automated-hate-speech-detection-and-the-problem-of-offensive-language.-in-proceedings-of-the-11th-icwsm,-pages-1–4.) ([2017](#thomas-davidson,-dana-warmsley,-michael-macy,-and-ingmar-weber.-2017.-automated-hate-speech-detection-and-the-problem-of-offensive-language.-in-proceedings-of-the-11th-icwsm,-pages-1–4.)), who define hate as content that is ‘derogatory’, [Waseem and Hovy](#zeerak-waseem-and-dirk-hovy.-2016.-hateful-sym--bols-or-hateful-people?-predictive-features-for-hate-speech-detection-on-twitter.-in-naacl-hlt,-pages-88–93.) ([2016](#zeerak-waseem-and-dirk-hovy.-2016.-hateful-sym--bols-or-hateful-people?-predictive-features-for-hate-speech-detection-on-twitter.-in-naacl-hlt,-pages-88–93.)) who include ‘attacks’ in their definition, and [Zampieri et al.](#marcos-zampieri,-shervin-malmasi,-preslav-nakov,-sara-rosenthal,-noura-farra,-and-ritesh-kumar.-2019.-predicting-the-type-and-target-of-offensive-posts-in-social-media.-in-proceedings-of-naacl-hlt-2019,-volume-1,-pages-1415–1420.) ([2019](#marcos-zampieri,-shervin-malmasi,-preslav-nakov,-sara-rosenthal,-noura-farra,-and-ritesh-kumar.-2019.-predicting-the-type-and-target-of-offensive-posts-in-social-media.-in-proceedings-of-naacl-hlt-2019,-volume-1,-pages-1415–1420.)) who include ‘insults’.

**Animosity** Content which expresses abuse against a group in an implicit or subtle manner. It is similar to the ‘implicit’ and ‘covert’ categories used in other taxonomies ([Waseem et al.](#zeerak-waseem,-thomas-davidson,-dana-warmsley,-and-ingmar-weber.-2017.-understanding-abuse:-a-typology-of-abusive-language-detection-subtasks.-in-proceedings-of-the-first-workshop-on-abusive-language-online,-pages-78–84.), [2017](#zeerak-waseem,-thomas-davidson,-dana-warmsley,-and-ingmar-weber.-2017.-understanding-abuse:-a-typology-of-abusive-language-detection-subtasks.-in-proceedings-of-the-first-workshop-on-abusive-language-online,-pages-78–84.); [Vidgen and Yasseri](#bertie-vidgen-and-taha-yasseri.-2019.-detecting-weak-and-strong-islamophobic-hate-speech-on-social-me--dia.-journal-of-information-technology-&-politics,-17\(1\):66–78.), [2019](#bertie-vidgen-and-taha-yasseri.-2019.-detecting-weak-and-strong-islamophobic-hate-speech-on-social-me--dia.-journal-of-information-technology-&-politics,-17\(1\):66–78.); [Kumar et al.](#ritesh-kumar,-atul-kr-ojha,-shervin-malmasi,-and-marcos-zampieri.-2018.-benchmarking-aggression-identification-in-social-media.-in-proceedings-ofthe-first-workshop-on-trolling,-aggression-and-cyber--bullying,,-1,-pages-1–11.), [2018](#ritesh-kumar,-atul-kr-ojha,-shervin-malmasi,-and-marcos-zampieri.-2018.-benchmarking-aggression-identification-in-social-media.-in-proceedings-ofthe-first-workshop-on-trolling,-aggression-and-cyber--bullying,,-1,-pages-1–11.)).

**Threatening language** Content which expresses intention to, support for, or encourages inflicting harm on a group, or identified members of the group. This category is used in datasets by [Ham-](#hugo-lewi-hammer.-2014.-detecting-threats-of-vio--lence-in-online-discussions-using-bigrams-of-impor--tant-words.-in-proceedings---2014-ieee-joint-intel--ligence-and-security-informatics-conference,-jisic-2014,-page-319.) [mer](#hugo-lewi-hammer.-2014.-detecting-threats-of-vio--lence-in-online-discussions-using-bigrams-of-impor--tant-words.-in-proceedings---2014-ieee-joint-intel--ligence-and-security-informatics-conference,-jisic-2014,-page-319.) ([2014](#hugo-lewi-hammer.-2014.-detecting-threats-of-vio--lence-in-online-discussions-using-bigrams-of-impor--tant-words.-in-proceedings---2014-ieee-joint-intel--ligence-and-security-informatics-conference,-jisic-2014,-page-319.)), [Golbeck et al.](#jennifer-golbeck,-alicia-a.-geller,-jayanee-thanki,-shalmali-naik,-kelly-m.-hoffman,-derek-michael-wu,-alexandra-berlinger,-priyanka-vengataraman,-shivika-khare,-zahra-ashktorab,-marianna-j.-martindale,-gaurav-shahane,-paul-cheakalos,-jenny-hottle,-siddharth-bhagwan,-raja-rajan-gu--nasekaran,-rajesh-kumar-gnanasekaran,-rashad-o.-banjo,-piyush-ramachandran,-lisa-rogers,-kris--tine-m.-rogers,-quint-gergory,-heather-l.-nixon,-meghna-sardana-sarin,-zijian-wan,-cody-buntain,-ryan-lau,-and-vichita-jienjitlert.-2017.-a-large-la--beled-corpus-for-online-harassment-research.-in-proceedings-of-the-acm-conference-on-web-science,-pages-229–233.) ([2017](#jennifer-golbeck,-alicia-a.-geller,-jayanee-thanki,-shalmali-naik,-kelly-m.-hoffman,-derek-michael-wu,-alexandra-berlinger,-priyanka-vengataraman,-shivika-khare,-zahra-ashktorab,-marianna-j.-martindale,-gaurav-shahane,-paul-cheakalos,-jenny-hottle,-siddharth-bhagwan,-raja-rajan-gu--nasekaran,-rajesh-kumar-gnanasekaran,-rashad-o.-banjo,-piyush-ramachandran,-lisa-rogers,-kris--tine-m.-rogers,-quint-gergory,-heather-l.-nixon,-meghna-sardana-sarin,-zijian-wan,-cody-buntain,-ryan-lau,-and-vichita-jienjitlert.-2017.-a-large-la--beled-corpus-for-online-harassment-research.-in-proceedings-of-the-acm-conference-on-web-science,-pages-229–233.)) and [Anzovino](#maria-anzovino,-elisabetta-fersini,-and-paolo-rosso.-2018.-automatic-identification-and-classification-of-misogynistic-language-on-twitter.-in-nldb,-pages-57–64.) [et al.](#maria-anzovino,-elisabetta-fersini,-and-paolo-rosso.-2018.-automatic-identification-and-classification-of-misogynistic-language-on-twitter.-in-nldb,-pages-57–64.) ([2018](#maria-anzovino,-elisabetta-fersini,-and-paolo-rosso.-2018.-automatic-identification-and-classification-of-misogynistic-language-on-twitter.-in-nldb,-pages-57–64.)).

**Support for hateful entities** Content which ex- plicitly glorifies, justifies or supports hateful ac- tions, events, organizations, tropes and individu- als (collectively, ‘entities’).

**Dehumanization** Content which ‘perceiv\[es\] or treat\[s\] people as less than human’ ([Haslam and](#n.-haslam-and-m.-stratemeyer.-2016.-recent-research-on-dehumanization.-current-opinion-in-psychology,-11:25–29.) [Stratemeyer](#n.-haslam-and-m.-stratemeyer.-2016.-recent-research-on-dehumanization.-current-opinion-in-psychology,-11:25–29.), [2016](#n.-haslam-and-m.-stratemeyer.-2016.-recent-research-on-dehumanization.-current-opinion-in-psychology,-11:25–29.)). It often involves describing groups as leeches, cockroaches, insects, germs or rats ([Mendelsohn et al.](#julia-mendelsohn,-yulia-tsvetkov,-and-dan-jurafsky.-2020.-a-framework-for-the-computational-linguis--tic-analysis-of-dehumanization.-frontiers-in-artifi--cial-intelligence,-3\(august\):1–24.), [2020](#julia-mendelsohn,-yulia-tsvetkov,-and-dan-jurafsky.-2020.-a-framework-for-the-computational-linguis--tic-analysis-of-dehumanization.-frontiers-in-artifi--cial-intelligence,-3\(august\):1–24.)).

2. ## **Targets of hate**

Hate can be targeted against any vulnerable, marginalized or discriminated-against group. We provided annotators with a non-exhaustive list of 29 identities to focus on (e.g., women, black people, Muslims, Jewish people and gay people), as well

as a small number of intersectional variations (e.g., ‘Muslim women’). They are given in Appendix [A](#list-of-identities). Some identities were considered out-of-scope for Hate, including men, white people, and heterosex- uals.

4. # **Annotation**

Data was annotated using an open-source web platform for dynamic dataset creation and model benchmarking.[3](#3https://anonymized-url) The platform supports human-and- model-in-the-loop dataset creation for a variety of NLP tasks. Annotation was overseen by two ex- perts in online hate. The annotation process is described in the following section. Annotation guidelines were created at the start of the project and then updated after each round in response to the increased need for detail from annotators. We followed the guidance for protecting and monitor- ing annotator well-being provided by [Vidgen et al.](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.) ([2019a](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.)). 20 annotators were recruited. They re- ceived extensive training and feedback during the project. Full details on the annotation team are given in Appendix [E](#data-statement). The small pool of annota- tors was driven by the logistical constraints of hir- ing and training them to the required standard and protecting their welfare given the sensitivity and complexity of the topic. Nonetheless, it raises the potential for bias. We take steps to address this in our test set construction and provide an annotator ID with each entry in our publicly-released dataset to enable further research into this issue.

5. # **Dataset formation**

The dataset was generated over four rounds, each of which involved ∼10*,* 000 entries. The final dataset comprises 41*,* 255 entries, as shown in Table [1](#label). The ten groups that are targeted most often are given in Table [2](#target-number-of-entries). Entries could target multiple groups. After each round, the data was split into training, dev and test splits of 80%, 10% and 10%, respec- tively. Approximately half of the entries in the test sets are produced by annotators who do not appear in the training and dev sets (between 1 and 4 in each round). This makes the test sets more chal- lenging and minimizes the risk of annotator bias given our relatively small pool of annotators ([Geva](#mor-geva,-yoav-goldberg,-and-jonathan-berant.-2019.-are-we-modeling-the-task-or-the-annotator?-an-inves--tigation-of-annotator-bias-in-natural-language-under--standing-datasets.-in-proceedings-of-the-2019-con--ference-on-empirical-methods-in-natural-language-processing-and-the-9th-international-joint-confer--ence-on-natural-language-processing-\(emnlp--ijcnlp\),-pages-1161–1166,-hong-kong,-china.-as--sociation-for-computational-linguistics.) [et al.](#mor-geva,-yoav-goldberg,-and-jonathan-berant.-2019.-are-we-modeling-the-task-or-the-annotator?-an-inves--tigation-of-annotator-bias-in-natural-language-under--standing-datasets.-in-proceedings-of-the-2019-con--ference-on-empirical-methods-in-natural-language-processing-and-the-9th-international-joint-confer--ence-on-natural-language-processing-\(emnlp--ijcnlp\),-pages-1161–1166,-hong-kong,-china.-as--sociation-for-computational-linguistics.), [2019](#mor-geva,-yoav-goldberg,-and-jonathan-berant.-2019.-are-we-modeling-the-task-or-the-annotator?-an-inves--tigation-of-annotator-bias-in-natural-language-under--standing-datasets.-in-proceedings-of-the-2019-con--ference-on-empirical-methods-in-natural-language-processing-and-the-9th-international-joint-confer--ence-on-natural-language-processing-\(emnlp--ijcnlp\),-pages-1161–1166,-hong-kong,-china.-as--sociation-for-computational-linguistics.)). The other half of each test set consists of content from annotators who do appear in the training and dev sets.

3[https://anonymized-url](https://anonymized-url/)

Rounds 2, 3 and 4 contain perturbations. In 18 cases the perturbation does not flip the label. This mistake was only identified after completion of the paper and is left in the dataset. These cases can be identified by checking whether original and perturbed entries that have been linked together have the same labels (e.g., whether an original and perturbation are both assigned to ’Hate’).

**Target model implementation** Every round has a model in the loop, which we call the ‘target model’. The target model is always trained on a combination of data collected in the previous round(s). For instance, **M2** is the target model used in **R2**, and was trained on **R1** and **R0** data. For consistency, we use the same model architec- ture everywhere, specifically RoBERTa ([Liu et al.](#yinhan-liu,-myle-ott,-naman-goyal,-jingfei-du,-man--dar-joshi,-danqi-chen,-omer-levy,-mike-lewis,-luke-zettlemoyer,-and-veselin-stoyanov.-2019.-roberta:-a-robustly-optimized-bert-pretraining-ap--proach.-arxiv:1907.11692.), [2019](#yinhan-liu,-myle-ott,-naman-goyal,-jingfei-du,-man--dar-joshi,-danqi-chen,-omer-levy,-mike-lewis,-luke-zettlemoyer,-and-veselin-stoyanov.-2019.-roberta:-a-robustly-optimized-bert-pretraining-ap--proach.-arxiv:1907.11692.)) with a sequence classification head. We use the implementation from the Transformers ([Wolf](#thomas-wolf,-lysandre-debut,-victor-sanh,-julien-chaumond,-clement-delangue,-anthony-moi,-pier--ric-cistac,-tim-rault,-r’emi-louf,-morgan-funtow--icz,-and-jamie-brew.-2019.-huggingface’s-trans--formers:-state-of-the-art-natural-language-process--ing.-arxiv,-abs/1910.03771.) [et al.](#thomas-wolf,-lysandre-debut,-victor-sanh,-julien-chaumond,-clement-delangue,-anthony-moi,-pier--ric-cistac,-tim-rault,-r’emi-louf,-morgan-funtow--icz,-and-jamie-brew.-2019.-huggingface’s-trans--formers:-state-of-the-art-natural-language-process--ing.-arxiv,-abs/1910.03771.), [2019](#thomas-wolf,-lysandre-debut,-victor-sanh,-julien-chaumond,-clement-delangue,-anthony-moi,-pier--ric-cistac,-tim-rault,-r’emi-louf,-morgan-funtow--icz,-and-jamie-brew.-2019.-huggingface’s-trans--formers:-state-of-the-art-natural-language-process--ing.-arxiv,-abs/1910.03771.)) library. More details are available in appendix [D](#model,-training,-and-evaluation-details).

For each new target model, we identify the best sampling ratio of previous rounds’ data using the dev sets. **M1** is trained on **R0** data. **M2** is trained on **R0** data and **R1** upsampled to a factor of five. **M3** is trained on the data used for **M2** and **R2** data upsampled to a factor of one hundred. **M4** is trained on the data used for **M3** and one lot of the **R3** data.

1. ## **Round 1 (R1)**

The target model in **R1** is **M1**, a RoBERTa model trained on **R0** which consists of 11 English lan- guage training datasets for hate and toxicity taken from [hatespeechdata.com](http://hatespeechdata.com/), as reported in [Vidgen](#bertie-vidgen-and-leon-derczynski.-2020.-directions-in-abusive-language-training-data:-garbage-in,-garbage-out.-plos-one,-pages-1–26.) [and Derczynski](#bertie-vidgen-and-leon-derczynski.-2020.-directions-in-abusive-language-training-data:-garbage-in,-garbage-out.-plos-one,-pages-1–26.) ([2020](#bertie-vidgen-and-leon-derczynski.-2020.-directions-in-abusive-language-training-data:-garbage-in,-garbage-out.-plos-one,-pages-1–26.)). It includes widely-used datasets provided by [Waseem](#zeerak-waseem.-2016.-are-you-a-racist-or-am-i-seeing-things?-annotator-influence-on-hate-speech-detection-on-twitter.-in-proceedings-of-the-first-workshop-on-nlp-and-computational-social-science,-pages-138–-142,-austin,-texas.-association-for-computational-linguistics.) ([2016](#zeerak-waseem.-2016.-are-you-a-racist-or-am-i-seeing-things?-annotator-influence-on-hate-speech-detection-on-twitter.-in-proceedings-of-the-first-workshop-on-nlp-and-computational-social-science,-pages-138–-142,-austin,-texas.-association-for-computational-linguistics.)), [Davidson](#thomas-davidson,-dana-warmsley,-michael-macy,-and-ingmar-weber.-2017.-automated-hate-speech-detection-and-the-problem-of-offensive-language.-in-proceedings-of-the-11th-icwsm,-pages-1–4.) [et al.](#thomas-davidson,-dana-warmsley,-michael-macy,-and-ingmar-weber.-2017.-automated-hate-speech-detection-and-the-problem-of-offensive-language.-in-proceedings-of-the-11th-icwsm,-pages-1–4.) ([2017](#thomas-davidson,-dana-warmsley,-michael-macy,-and-ingmar-weber.-2017.-automated-hate-speech-detection-and-the-problem-of-offensive-language.-in-proceedings-of-the-11th-icwsm,-pages-1–4.)) and [Founta et al.](#antigoni-maria-founta,-constantinos-djouvas,-de--spoina-chatzakou,-ilias-leontiadis,-jeremy-black--burn,-gianluca-stringhini,-athena-vakali,-michael-sirivianos,-and-nicolas-kourtellis.-2018.-large-scale-crowdsourcing-and-characterization-of-twit--ter-abusive-behavior.-in-icwsm,-pages-1–11.) ([2018](#antigoni-maria-founta,-constantinos-djouvas,-de--spoina-chatzakou,-ilias-leontiadis,-jeremy-black--burn,-gianluca-stringhini,-athena-vakali,-michael-sirivianos,-and-nicolas-kourtellis.-2018.-large-scale-crowdsourcing-and-characterization-of-twit--ter-abusive-behavior.-in-icwsm,-pages-1–11.)). It comprises 468*,* 928 entries, of which 22% are hateful/toxic. The dataset was anonymized by replacing user- names, indicated by the ‘@’ symbol. URLs were also replaced with a special token. In **R1**, annota- tors were instructed to enter synthetic content into the model that would trick **M1** using their own cre- ativity and by exploiting any model weaknesses they identified through the real-time feedback.

All entries were validated by one other annota- tor and entries marked as incorrect were sent for review by expert annotators. This happened with 1*,* 011 entries. 385 entries were excluded for being entirely incorrect. In the other cases, the expert an- notator decided the final label and/or made minor

| Label | Type | Total | R1	R2	R3	R4 |
| :---- | :---- | :---- | :---- |

| Hate | Not given | 7*,* 197 | 7*,* 197 | 0 | 0 | 0 |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
|  | Animosity | 3*,* 439 | 0 | 758 | 1*,* 206 | 1*,* 475 |
|  | Dehumanization | 906 | 0 | 261 | 315 | 330 |
|  | Derogation | 9*,* 907 | 0 | 3*,* 574 | 3*,* 206 | 3*,* 127 |
|  | Support | 207 | 0 | 41 | 104 | 62 |
|  | Threatening | 606 | 0 | 376 | 148 | 82 |

|  | Total | 22*,* 262 | 7*,* 197	5*,* 010	4*,* 979	5*,* 076 |
| :---- | :---- | :---- | :---- |

| Not Hate | / | 18*,* 993 | 3*,* 960	4*,* 986	4*,* 971	5*,* 076 |
| :---- | :---- | :---- | :---- |

| All | TOTAL | 41*,* 255 | 11*,* 157	9*,* 996	9*,* 950	10*,* 152 |
| :---- | :---- | :---- | :---- |

Table 1: Summary of data collected in each round

 **Target	Number of entries** 

| Black people | 2*,* 278 |
| :---- | :---: |
| Women | 2*,* 192 |
| Jewish people | 1*,* 293 |
| Muslims | 1*,* 144 |
| Trans people | 972 |
| Gay people | 875 |
| Immigrants | 823 |
| Disabled people | 575 |
| Refugees | 533 |
| Arabs | 410 |

Table 2: Most common targets of hate in the dataset

adjustments to the text. The final dataset comprises 11*,* 157 entries of which 7*,* 197 are ‘Hate’ (65%) and 3*,* 960 are ‘Not Hate’ (35%). The type and target of Hate was not recorded by annotators in **R1**.

2. ## **Round 2 (R2)**

A total of 9*,* 996 entries were entered in **R2**. The hateful entries are split between Derogation (3*,* 577, 72%), Dehumanization (255, 5%), Threats (380, 8%), Support for hateful entities (39, 1%) and An- imosity (759, 15%). In **R2** we gave annotators adversarial ‘pivots’ to guide their work, which we identified from a review of previous literature (see Section [2](#background)). The 10 hateful and 12 not hateful adver- sarial pivots, with examples and a description, are given in Appendix [B](#list-of-pivots-in-r2). Half of **R2** comprises origi- nally entered content and the other half comprises perturbed contrast sets.

Following [Gardner et al.](#matt-gardner,-yoav-artzi,-victoria-basmova,-jonathan-berant,-ben-bogin,-sihao-chen,-pradeep-dasigi,-dheeru-dua,-yanai-elazar,-ananth-gottumukkala,-nitish-gupta,-hanna-hajishirzi,-gabriel-ilharco,-daniel-khashabi,-kevin-lin,-jiangming-liu,-nel--son-f.-liu,-phoebe-mulcaire,-qiang-ning,-sameer) ([2020](#matt-gardner,-yoav-artzi,-victoria-basmova,-jonathan-berant,-ben-bogin,-sihao-chen,-pradeep-dasigi,-dheeru-dua,-yanai-elazar,-ananth-gottumukkala,-nitish-gupta,-hanna-hajishirzi,-gabriel-ilharco,-daniel-khashabi,-kevin-lin,-jiangming-liu,-nel--son-f.-liu,-phoebe-mulcaire,-qiang-ning,-sameer)), perturbations were created offline without feedback from a model-in-the-loop. Annotators were given four  
main points of guidance: (1) ensure perturbed en-

tries are realistic, (2) firmly meet the criteria of the flipped label and type, (3) maximize diversity within the dataset in terms of type, target and how entries are perturbed and (4) make the least changes possible while meeting (1), (2) and (3). Common strategies for perturbing entries included changing the target (e.g., from ‘black people’ to ‘the local council’), changing the sentiment (e.g. ‘It’s *won- derful* having gay people round here’), negating an attack (e.g. ‘Muslims are *not* a threat to the UK’) and quoting or commenting on hate.

Of the original entries, those which fooled **M1** were validated by between three and five other an- notators. Every perturbation was validated by one other annotator. Annotators could select: (1) cor- rect if they agreed with the label and, for Hate, the type/target, (2) incorrect if the label was wrong or

(3) flag if they thought the entry was unrealistic and/or they agreed with the label for hate but dis- agreed with the type or target. Krippendorf’s alpha is 0*.*815 for all original entries if all ‘flagged’ en- tries are treated as ‘incorrect’, indicating extremely high levels of agreement ([Hallgren](#kevin-a-hallgren.-2012.-computing-inter-rater-re--liability-for-observational-data:-an-overview-and-tutorial.-tutorials-in-quantitative-methods-for-psy--chology,-8\(1\):23–34.), [2012](#kevin-a-hallgren.-2012.-computing-inter-rater-re--liability-for-observational-data:-an-overview-and-tutorial.-tutorials-in-quantitative-methods-for-psy--chology,-8\(1\):23–34.)). All of the original entries identified by at least two validators as incorrect/flagged, and perturbations which were identified by one validator as incor- rect/flagged, were sent for review by an expert an- notator. This happened in 760 cases in this round.

**Lessons from R2** The validation and review pro- cess identified some limitations of the **R2** dataset. First, several ‘template’ statements were entered by annotators. These are entries which have a standardized syntax and/or lexicon, with only the identity changed, such as ‘\[Identity\] are \[negative attribute\]’. When there are many cases of each tem-

plate they are easy for the model to correctly clas- sify because they create a simple decision bound- ary. Discussion sessions showed that annotators used templates (i) to ensure coverage of different identities (an important consideration in making a *generalisable* online hate classifier) and (ii) to maximally exploit model weaknesses to increase their model error rate. We banned the use of tem- plates. Second, in attempting to meet the ‘pivots’ they were assigned, some annotators created unre- alistic entries. We updated guidance to emphasize the importance of realism. Third, the pool of 10 trained annotators is large for a project annotating online hate but annotator biases were still produced. Model performance was high in **R2** when evalu- ated on a training/dev/test split with all annotators stratified. We then held out some annotators’ con- tent and performance dropped substantially. We use this setup for all model evaluations.

3. ## **Round 3 (R3)**

In **R3** annotators were tasked with finding real- world hateful online content to inspire their entries. All real-world content was subject to at least one substantial adjustment prior to being presented to the model. 9*,* 950 entries were entered in **R3**. The hateful entries are split between Derogation (3*,* 205, 64%), Dehumanization (315, 6%), Threats (147,

3%), Support for hateful entities (104, 2%) and Animosity (1*,* 210, 24%). Half of **R3** comprises originally entered content (4*,* 975) and half com- prises perturbed contrast sets (4*,* 975).

The same validation procedure was used as with **R2**. Krippendorf’s alpha was 0*.*55 for all origi- nal entries if all ‘flagged’ entries are treated as ‘incorrect’, indicating moderate levels of agree- ment ([Hallgren](#kevin-a-hallgren.-2012.-computing-inter-rater-re--liability-for-observational-data:-an-overview-and-tutorial.-tutorials-in-quantitative-methods-for-psy--chology,-8\(1\):23–34.), [2012](#kevin-a-hallgren.-2012.-computing-inter-rater-re--liability-for-observational-data:-an-overview-and-tutorial.-tutorials-in-quantitative-methods-for-psy--chology,-8\(1\):23–34.)). This is lower than **R2**, but still comparable with other hate speech datasets (e.g., [Wulczyn et al.](#ellery-wulczyn,-nithum-thain,-and-lucas-dixon.-2017b.-ex-machina:-personal-attacks-seen-at-scale.-in-proceedings-of-the-international-world-wide-web-conference,-pages-1391–1399.) ([2017b](#ellery-wulczyn,-nithum-thain,-and-lucas-dixon.-2017b.-ex-machina:-personal-attacks-seen-at-scale.-in-proceedings-of-the-international-world-wide-web-conference,-pages-1391–1399.)) achieve Krippnedorf’s alpha of 0*.*45). Note that more content is labelled as Animosity compared with **R2** (24% compared with 15%), which tends to have higher levels of disagreement. 981 entries were reviewed by the expert annotators.

4. ## **Round 4 (R4)**

As with **R3**, annotators searched for real-world hateful online content to inspire their entries. In addition, each annotator was given a target iden- tity to focus on (e.g., Muslims, women, Jewish people). The annotators (i) investigated hateful on- line forums and communities relevant to the target

identity to find the most challenging and nuanced content and (ii) looked for challenging non-hate examples, such as neutral discussions of the iden- tity. 10*,* 152 entries were entered in **R4**, comprising 5*,* 076 ‘Hate’ and 5*,* 076 ‘Not Hate’. The hateful entries are split between Derogation (3*,* 128, 62%), Dehumanization (331, 7%), Threats (82, 2%), Sup- port for hateful entities (61, 1%) and Animosity (1*,* 474, 29%). Half of **R4** comprises originally en- tered content (5*,* 076) and half comprises perturbed contrast sets (5*,* 076). The same validation pro- cedure was used as in **R2** and **R3**. Krippendorf’s alpha was 0*.*52 for all original entries if all ‘flagged’ entries are treated as ‘incorrect’, indicating mod- erate levels of agreement ([Hallgren](#kevin-a-hallgren.-2012.-computing-inter-rater-re--liability-for-observational-data:-an-overview-and-tutorial.-tutorials-in-quantitative-methods-for-psy--chology,-8\(1\):23–34.), [2012](#kevin-a-hallgren.-2012.-computing-inter-rater-re--liability-for-observational-data:-an-overview-and-tutorial.-tutorials-in-quantitative-methods-for-psy--chology,-8\(1\):23–34.)). This is similar to **R2**. 967 entries were reviewed by the expert annotators following the validation process.

6. # **Model performance**

In this section, we examine the performance of models on the collected data, both when used in- the-loop during data collection (measured by the model error rate on new content shown by annota- tors), as well as when separately evaluated against the test sets in each round’s data. We also examine how models generalize by evaluating them on the out-of-domain suite of diagnostic functional tests in HATECHECK.

1. ## **Model error rate**

The model error rate is the rate at which annotator- generated content tricks the model. It decreases as the rounds progress, as shown in Table [3](#round). **M1**, which was trained on a large set of public hate speech datasets, was the most easily tricked, even though many annotators were learning and had not been given advice on its weaknesses. 54*.*7% of en- tries tricked it, including 64*.*6% of Hate and 49*.*2% of Not Hate. Only 27*.*7% of content tricked the final model (**M4**), including 23*.*7% of Hate and 31*.*7% of Not Hate. The type of hate affected how frequently entries tricked the model. In general, more explicit and overt forms of hate had the low- est model error rates, with threatening language and dehumanization at 18*.*2% and 24*.*8% on aver- age, whereas support for hateful entities and an- imosity had the highest error (55*.*4% and 46*.*4% respectively). The model error rate falls as the rounds progress but nonetheless this metric poten- tially still underestimates the increasing difficulty of the rounds and the improvement in the models.

| Round | Total | Not | Hate | Animosity	Dehuman	Derogation	Support	Threatening \-ization |
| :---- | :---- | :---- | :---- | :---- |

| R1 | 54*.*7% | 64*.*6% | 49*.*2% | \- | \- | \- | \- | \- |
| :---- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| R2 | 34*.*3% | 38*.*9% | 29*.*7% | 40*.*1% | 25*.*5% | 28*.*7% | 53*.*8% | 18*.*4% |
| R3 | 27*.*8% | 20*.*5% | 35*.*1% | 53*.*8% | 27*.*9% | 29*.*2% | 59*.*6% | 17*.*7% |
| R4 | 27*.*7% | 23*.*7% | 31*.*7% | 44*.*5% | 21*.*1% | 26*.*9% | 49*.*2% | 18*.*3% |

| All | 36*.*6% | 35*.*4% | 37*.*7% | 46*.*4% | 24*.*8% | 28*.*3% | 55*.*4% | 18*.*2% |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |

Table 3: Error rate for target models in each round. Error rate decreases as the rounds progress, indicating that models become harder to trick. Annotators were not given real-time feedback on whether their entries tricked the model when creating perturbations. More information about tuning is available in appendix [D](#model,-training,-and-evaluation-details)

Annotators became more experienced and skilled over the annotation process, and entered progres- sively more adversarial content. As such the con- tent that annotators enter becomes far harder to classify in the later rounds, which is also reflected

**Target model performance on HateCheck**

100%

75%

Label  
in all models’ lower performance on the later round test sets (see Table [4](#model)).

2. ## **Test set performance**

Table [4](#model) shows the macro F1 of models trained on different combinations of data, evaluated on the test sets from each round (see Appendix [C](#development-set-performance) for dev set performance). The target models achieve lower

50%

25%

0%

M1	M2	M3	M4

Target models

Hate All None

scores when evaluated on test sets from the later rounds, demonstrating that the dynamic approach to data collection leads to increasingly more chal- lenging data. The highest scores for **R3** and **R4** data are in the mid-70s, compared to the high 70s in **R2** and low 90s in **R1**. Generally, the target mod- els from the later rounds have higher performance across the test sets. For instance, **M4** is the best per- forming model on **R1**, **R2** and **R4** data. It achieves

75.97 on the **R4** data whereas **M3** achieves 74.83 and **M2** only 60.87. A notable exception is **M1** which outperforms **M2** on the **R3** and **R4** test sets. Table [4](#model) presents the results for models trained on just the training sets from each round (with no upsampling), indicated by M(RX only). In general the performance is lower than the equivalent target model. For instance, **M4** achieves macro F1 of  
75.97 on the **R4** test data. **M(R3 only)** achieves

73.16 on that test set and **M(R4 only)** just 69.6. In other cases, models which are trained on just one round perform well on some rounds but are far worse on others. Overall, building models cumu- latively leads to more consistent performance. Ta- ble [4](#model) also shows models trained on the cumulative rounds of data with no upsampling, indicated by M(RX+RY). In general, performance is lower with-

Figure 1: Performance of target models on the HATE\- CHECK test suite.

out upsampling; the F1 of **M3** is 2 points higher on the **R3** test set than the equivalent non-upsampled model (**M(R0+R1+R2)**).

3. ## **HateCheck**

To better understand the weaknesses of the target models from each round, we apply them to HAT\- ECHECK, as presented by [Ro¨ttger et al.](#paul-ro¨ttger,-bertram-vidgen,-dong-nguyen,-zeerak-waseem,-helen-margetts,-and-janet-pierrehumbert.-2020.-hatecheck:-functional-tests-for-hate-speech-detection-models.) ([2020](#paul-ro¨ttger,-bertram-vidgen,-dong-nguyen,-zeerak-waseem,-helen-margetts,-and-janet-pierrehumbert.-2020.-hatecheck:-functional-tests-for-hate-speech-detection-models.)). HATECHECK is a suite of functional tests for hate speech detection models, based on the test- ing framework introduced by [Ribeiro et al.](#marco-tulio-ribeiro,-tongshuang-wu,-carlos-guestrin,-and-sameer-singh.-2020.-beyond-accuracy:-be--havioral-testing-of-nlp-models-with-checklist.-in-proceedings-of-the-58th-annual-meeting-of-the-asso--ciation-for-computational-linguistics,-pages-4902–-4912,-online.-association-for-computational-lin--guistics.) ([2020](#marco-tulio-ribeiro,-tongshuang-wu,-carlos-guestrin,-and-sameer-singh.-2020.-beyond-accuracy:-be--havioral-testing-of-nlp-models-with-checklist.-in-proceedings-of-the-58th-annual-meeting-of-the-asso--ciation-for-computational-linguistics,-pages-4902–-4912,-online.-association-for-computational-lin--guistics.)). It comprises 29 tests, of which 18 correspond to distinct expressions of hate and the other 11 are non-hateful contrasts. The selection of functional tests is motivated by a review of previous literature and interviews with 21 NGO workers. From the 29 tests in the suite, 3*,* 728 labelled entries are gen- erated in the dataset of which 69% are ‘Hate’ and 31% are ‘Not Hate’.

Performance of target models trained on later rounds is substantially higher, increasing from 60% (on both ‘Hate’ and ‘Not Hate’) combined for **M1**

| Model | R1 | R2 | R3 | R4 |
| :---- | :---: | :---: | :---: | :---: |
| M1 (R1 Target) | 44.84±1.1 | 54.42±0.45 | 66.07±1.03 | 60.91±0.4 |
| M2 (R2 Target) | 90.17±1.42 | 66.05±0.67 | 62.89±1.26 | 60.87±1.62 |
| M3 (R3 Target) | 91.37±1.26 | 77.14±1.26 | **76.97**±**0.49** | 74.83±0.92 |
| M4 (R4 Target) | **92.01**±**0.6** | **78.02**±**0.91** | 75.89±0.62 | **75.97**±**0.96** |
| M(R1 only) | **92.20**±**0.55** | 62.87±0.63 | 47.67±1.04 | 52.37±1.27 |
| M(R2 only) | 80.73±0.4 | 76.52±0.7 | **77.43**±**0.51** | **74.88**±**0.85** |
| M(R3 only) | 72.71±1.05 | **78.55**±**0.71** | 74.14±1.5 | 73.16±0.58 |
| M(R4 only) | 72.26±1.3 | 76.78±1.65 | 77.21±0.43 | 69.6±0.6 |
| M(R0+R1) | 88.78±0.89 | 66.15±0.77 | 67.15±1.11 | 63.44±0.26 |
| M(R0+R1+R2) | 91.09±0.37 | 74.73±0.95 | 74.73±0.46 | 71.59±0.59 |
| M(R0+R1+R2+R3) | **91.17**±**0.99** | 77.03±0.72 | 74.6±0.48 | **73.94**±**0.94** |
| M(R0+R1+R2+R3+R4) | 90.3±0.96 | **77.93**±**0.84** | **76.79**±**0.24** | 72.93±0.56 |

Table 4: Macro F1 with standard deviation over 5 training rounds, evaluated on each rounds’ test set. Early- stopping is performed on the latest dev set for each round where dev results are obtained at least once per epoch, out of four epochs.

to 95% for **M4**. Performance is better than all four models evaluated by [Ro¨ttger et al.](#paul-ro¨ttger,-bertram-vidgen,-dong-nguyen,-zeerak-waseem,-helen-margetts,-and-janet-pierrehumbert.-2020.-hatecheck:-functional-tests-for-hate-speech-detection-models.) ([2020](#paul-ro¨ttger,-bertram-vidgen,-dong-nguyen,-zeerak-waseem,-helen-margetts,-and-janet-pierrehumbert.-2020.-hatecheck:-functional-tests-for-hate-speech-detection-models.)), of which Perspective’s toxicity classifier[4](#4see:-https://www.perspectiveapi.com/#/-home.) is best perform- ing with 77% overall accuracy, including 90% on ‘Hate’ and 48% on ‘Not Hate’. Notably, the per- formance of **M4** is consistent across both ‘Hate’ and ‘Not Hate’, achieving 95% and 93% respec- tively. This is in contrast to earlier target models, such as **M2** which achieves 91% on ‘Hate’ but only 67% on ‘Not Hate’ (note that this is actually a *re- duction* in performance from **M1** on ‘Not Hate’). Note that HATECHECK only has negative predic- tive power. These results indicate the *absence* of particular weaknesses in models rather than neces- sarily characterising generalisable strengths.

A further caveat is that in **R2** the annotators were given adversarial pivots to improve their ability to trick the models (See above). These pivots exploit similar model weaknesses as the functional tests in HATECHECK expose, which creates a risk that this gold standard is not truly independent. We did not identify any exact matches, although after low- ering case and removing punctuation there are 21 matches. This is just 0.05% of our dataset but indi- cates a risk of potential overlap and cross-dataset similarity.

7. # **Discussion**

Online hate detection is a complex and nuanced problem, and creating systems that are accurate,

4See: [https://www.perspectiveapi.com/\#/](https://www.perspectiveapi.com/%23/home) [home](https://www.perspectiveapi.com/%23/home).

robust and generalisable across target, type and domain has proven difficult for AI-based solutions. It requires having datasets which are large, varied, expertly annotated and contain challenging content. Dynamic dataset generation offers a powerful and scalable way of creating these datasets, and training and evaluating more robust and high performing models. Over the four rounds of model training and evaluation we show that the performance of target models improves, as measured by their accuracy on the test sets. The robustness of the target models from later rounds also increases, as shown by their better performance on HATECHECK.

Dynamic data creation systems offer several advantages for training better performing mod- els. First, problems can be addressed as work is conducted – rather than creating the dataset and then discovering any inadvertent design flaws. For instance, we continually worked with annota- tors to improve their understanding of the guide- lines and strategies for tricking the model. We also introduced perturbations to ensure that content was more challenging. Second, annotators can in- put more challenging content because their work is guided by real-time feedback from the target model. Discussion sessions showed that annotators responded to the models’ feedback in each round, adjusting their content to find better ways to trick it. This process of people trying to find ways to cir- cumvent hate speech models such that their content goes undetected is something that happens often in the real world. Third, dynamic datasets can be

constructed to better meet the requirements of ma- chine learning; our dataset is balanced, comprising  
∼54% hate. It includes hate targeted against a large number of targets, providing variety for the model to learn from, and many entries were constructed to include known challenging content, such as use of slurs and identity referents.

However, our approach also presents some chal- lenges. First, it requires substantial infrastructure and resources. This project would not have been possible without the use of an online interface and a backend that can serve up state-of-the-art hate speech detection models with relatively low latency. Second, it requires substantial domain expertise from dataset creators as well as annotators, such as knowing where to find real-world hate to inspire synthetic entries. This requires a cross-disciplinary team, combining social science with linguistics and machine learning expertise. Third, evaluating and validating content in a time-constrained dynamic setting can introduce new pressures on the annota- tion process. The perturbation process also requires additional annotator training, or else might intro- duce other inadvertent biases.

8. # **Conclusion**

We presented a human-and-model-in-the-loop pro- cess for training an online hate detection system. It was employed dynamically to collect four rounds of hate speech datasets. The datasets are large and high quality, having been obtained using only expert annotators. They have fine-grained annota- tions for the type and target of hate, and include perturbations to increase the dataset difficulty. We demonstrated that the models trained on these dy- namically generated datasets are much better at the task of hate speech detection, including evaluation on out-of-domain functional test suites.

In future work we aim to expand the size and diversity of the annotator pool for further rounds of dynamic adversarial data collection. We would like to evaluate different models in-the-loop be- yond RoBERTa. The datasets also open many new avenues of investigation, including training models on only original entries and evaluating against per- turbations (and vice versa) and training multi-label results for type and target of hate. Data collection for future rounds is ongoing.

# **Impact Statement & Ethical Considerations**

In the Impact Statement we address relevant ethical considerations that were not explicitly discussed in the main body of the paper.

**Data** The entries in the dataset were created by the annotation team and, where needed, reviewed by the expert annotators. In no cases did annotators enter content that they found on online sites. All entries which were closely inspired by real-world content (e.g., data entered during round 4\) had sub- stantial adjustments made to them. As such, the data is synthetic.

**Annotator Compensation** We employed a team of twenty annotators to enter content who worked varying hours on a flexible basis over four months. Annotators were compensated at a rate of £16 per hour. The rate was set 50% above the local liv- ing wage (£10.85), even though all work was com- pleted remotely. All training time and meetings were paid.

**Intended Use** The approach, dataset and mod- els presented here are intended to support more accurate and robust detection and classification of online hate. We anticipate that the high-quality and fine-grained labels in the dataset will advance research in online hate in other ways, such as en- abling multiclass classification of types and targets of online hate.

**Potential Misuse** The dataset and models we present could in principle be used to train a genera- tive hate speech model. Alternatively, the dataset and models could be used to better understand the limitations of current detection tools and then at- tack them. For instance, if a malicious actor investi- gated our models then they could better understand what content tricks content moderation tools and then use this knowledge to avoid their content be- ing flagged on social media platforms. However, we believe that these outcomes are unlikely. We do not report any new weaknesses that have not been established in previous research, and the models we present still contain several limitations. Further, it is unlikely that a malicious actor would be able to train a powerful enough generative model from this dataset (given its size and composition) to affect their activities. Overall, the scientific and social benefits of the present research arguably outweighs the small risk of their misuse.

# **References**

Maria Anzovino, Elisabetta Fersini, and Paolo Rosso. 2018\. [Automatic identification and classification of](https://doi.org/10.1007/978-3-319-91947-8_6) [misogynistic language on Twitter](https://doi.org/10.1007/978-3-319-91947-8_6). In *NLDB*, pages 57–64.

Valerio Basile, Cristina Bosco, Elisabetta Fersini, Deb- ora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela Sanguinetti. 2019\. [Sem Eval-2019 Task 5: Multilingual Detection of](https://www.aclweb.org/anthology/S19-2007) [Hate Speech Against Immigrants and Women in](https://www.aclweb.org/anthology/S19-2007) [Twitter](https://www.aclweb.org/anthology/S19-2007). In *Proceedings of the 13th International Workshop on Semantic Evaluation*, pages 54–63.

Emily M. Bender and Batya Friedman. 2018\. [Data](https://doi.org/10.1162/tacl_a_00041) [statements for natural language processing: Toward](https://doi.org/10.1162/tacl_a_00041) [mitigating system bias and enabling better science](https://doi.org/10.1162/tacl_a_00041). *Transactions of the Association for Computational Linguistics*, 6:587–604.

Tommaso Caselli, Valerio Basile, Jelena Mitrovic´, Inga Kartoziya, and Michael Granitzer. 2020\. [I Feel Of-](https://sites.google.com/view/alw3/) [fended, Don’t Be Abusive\! Implicit/Explicit Mes-](https://sites.google.com/view/alw3/) [sages in Offensive and Abusive Language](https://sites.google.com/view/alw3/). In *Pro- ceedings of the 12th Conference on Language Re- sources and Evaluation (LREC)*, pages 6193–6202.

Yi-Ling Chung, Elizaveta Kuzmenko, Serra Sinem Tekiroglu, and Marco Guerini. 2019\. [CONAN \-](https://doi.org/10.18653/v1/p19-1271) [COunter NArratives through Nichesourcing: a Mul-](https://doi.org/10.18653/v1/p19-1271) [tilingual Dataset of Responses to Fight Online Hate](https://doi.org/10.18653/v1/p19-1271) [Speech](https://doi.org/10.18653/v1/p19-1271). In *Proceedings of the 57th Annual Meeting of the ACL*, pages 2819–2829.

Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017\. [Automated Hate Speech](http://arxiv.org/abs/1703.04009) [Detection and the Problem of Offensive Language](http://arxiv.org/abs/1703.04009). In *Proceedings of the 11th ICWSM*, pages 1–4.

Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. 2019\. Build it break it fix it for dialogue safety: Robustness from adversarial human attack. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing*, pages 4537–4546.

Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2018\. [Measuring and Miti-](https://doi.org/10.1145/3278721.3278729) [gating Unintended Bias in Text Classification](https://doi.org/10.1145/3278721.3278729). In *AAAI/ACM Conference on AI, Ethics, and Society*, pages 67–73.

Antigoni-maria Founta, Constantinos Djouvas, De- spoina Chatzakou, Ilias Leontiadis, Jeremy Black- burn, Gianluca Stringhini, Athena Vakali, Michael Sirivianos, and Nicolas Kourtellis. 2018\. [Large](http://arxiv.org/abs/1802.00393) [Scale Crowdsourcing and Characterization of Twit-](http://arxiv.org/abs/1802.00393) [ter Abusive Behavior](http://arxiv.org/abs/1802.00393). In *ICWSM*, pages 1–11.

Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hanna Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nel- son F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer

Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. 2020\. Evaluating NLP Models’ Local Decision Boundaries via Contrast Sets. In *Findings of the Assocation for Computational Linguistics: EMNLP 2020*, pages 1307–1323.

Sahaj Garg, Ankur Taly, Vincent Perot, Ed H. Chi, Nicole Limtiaco, and Alex Beutel. 2019\. [Counter-](https://doi.org/10.1145/3306618.3317950) [factual fairness in text classification through robust-](https://doi.org/10.1145/3306618.3317950) [ness](https://doi.org/10.1145/3306618.3317950). *Proceedings of the 2019 AAAI/ACM Confer- ence on AI, Ethics, and Society*, pages 219–226.

Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019\. [Are we modeling the task or the annotator? an inves-](https://doi.org/10.18653/v1/D19-1107) [tigation of annotator bias in natural language under-](https://doi.org/10.18653/v1/D19-1107) [standing datasets](https://doi.org/10.18653/v1/D19-1107). In *Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP)*, pages 1161–1166, Hong Kong, China. As- sociation for Computational Linguistics.

Jennifer Golbeck, Alicia A. Geller, Jayanee Thanki, Shalmali Naik, Kelly M. Hoffman, Derek Michael Wu, Alexandra Berlinger, Priyanka Vengataraman, Shivika Khare, Zahra Ashktorab, Marianna J. Martindale, Gaurav Shahane, Paul Cheakalos, Jenny Hottle, Siddharth Bhagwan, Raja Rajan Gu- nasekaran, Rajesh Kumar Gnanasekaran, Rashad O. Banjo, Piyush Ramachandran, Lisa Rogers, Kris- tine M. Rogers, Quint Gergory, Heather L. Nixon, Meghna Sardana Sarin, Zijian Wan, Cody Buntain, Ryan Lau, and Vichita Jienjitlert. 2017\. [A Large La-](https://doi.org/10.1145/3091478.3091509) [beled Corpus for Online Harassment Research](https://doi.org/10.1145/3091478.3091509). In *Proceedings of the ACM Conference on Web Science*, pages 229–233.

Tommi Gro¨ndahl, Luca Pajola, Mika Juuti, Mauro Conti, and N. Asokan. 2018\. All You Need is ”Love”: Evading Hate-speech Detection. In *Pro- ceedings of the 11th ACM Workshop on Artificial In- telligence and Security*, pages 2–12.

Kevin A Hallgren. 2012\. [Computing Inter-Rater Re-](https://doi.org/10.1016/j.biotechadv.2011.08.021.Secreted) [liability for Observational Data: An Overview and](https://doi.org/10.1016/j.biotechadv.2011.08.021.Secreted) [Tutorial.](https://doi.org/10.1016/j.biotechadv.2011.08.021.Secreted) *Tutorials in quantitative methods for psy- chology*, 8(1):23–34.

Hugo Lewi Hammer. 2014\. [Detecting threats of vio-](https://doi.org/10.1109/JISIC.2014.64) [lence in online discussions using bigrams of impor-](https://doi.org/10.1109/JISIC.2014.64) [tant words](https://doi.org/10.1109/JISIC.2014.64). In *Proceedings \- 2014 IEEE Joint Intel- ligence and Security Informatics Conference, JISIC 2014*, page 319\.

N. Haslam and M. Stratemeyer. 2016\. Recent research on dehumanization. *Current Opinion in Psychology*, 11:25–29.

Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017\. [Deceiving Google’s](http://arxiv.org/abs/1702.08138) [Perspective API Built for Detecting Toxic Com-](http://arxiv.org/abs/1702.08138) [ments](http://arxiv.org/abs/1702.08138). In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops*, pages 1305–1309.

Mladen Karan and Jan Sˇnajder. 2018\. [Cross-Domain](http://takelab.fer.hr/alfeda) [Detection of Abusive Language Online](http://takelab.fer.hr/alfeda). In *2nd Workshop on Abusive Language Online*, pages 132– 137\.

Divyansh Kaushik, Eduard Hovy, and Zachary C Lip- ton. 2019\. Learning the difference that makes a difference with counterfactually-augmented data. *arXiv preprint arXiv:1909.12434*.

Brendan Kennedy, Xisen Jin, Aida Mostafazadeh Da- vani, Morteza Dehghani, and Xiang Ren. 2020\. [Con-](https://doi.org/10.18653/v1/2020.acl-main.483) [textualizing Hate Speech Classifiers with Post-hoc](https://doi.org/10.18653/v1/2020.acl-main.483) [Explanation](https://doi.org/10.18653/v1/2020.acl-main.483). In *Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics*, pages 5435–5442.

Ritesh Kumar, Atul Kr Ojha, Shervin Malmasi, and Marcos Zampieri. 2018\. Benchmarking Aggression Identification in Social Media. In *Proceedings ofthe First Workshop on Trolling, Aggression and Cyber- bullying,*, 1, pages 1–11.

Jana Kurrek, Haji Mohammad Saleem, and Derek Ruths. 2020\. [Towards a Comprehensive Taxonomy](https://doi.org/10.18653/v1/2020.alw-1.17) [and Large-Scale Annotated Corpus for Online Slur](https://doi.org/10.18653/v1/2020.alw-1.17) [Usage](https://doi.org/10.18653/v1/2020.alw-1.17). In *Proceedings of the Fourth Workshop on Online Abuse and Harms*, pages 138–149.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019\. Roberta: A robustly optimized bert pretraining ap- proach. *arXiv:1907.11692*.

Thomas Mandl, Sandip Modha, Prasenjit Majumder, Daksh Patel, Mohana Dave, Chintak Mandlia, and Aditya Patel. 2019\. [Overview of the HASOC track](https://doi.org/10.1145/3368567.3368584) [at FIRE 2019: Hate speech and offensive content](https://doi.org/10.1145/3368567.3368584) [identification in Indo-European languages](https://doi.org/10.1145/3368567.3368584). In *FIRE ’19: Proceedings of the 11th Forum for Information Retrieval Evaluation*, pages 14–17.

Binny Mathew, Punyajoy Saha, Seid Muhie Yi- mam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. 2020\. [Hatexplain: A benchmark dataset](http://arxiv.org/abs/2012.10289) [for explainable hate speech detection](http://arxiv.org/abs/2012.10289).

Julia Mendelsohn, Yulia Tsvetkov, and Dan Jurafsky. 2020\. [A Framework for the Computational Linguis-](https://doi.org/10.3389/frai.2020.00055) [tic Analysis of Dehumanization](https://doi.org/10.3389/frai.2020.00055). *Frontiers in Artifi- cial Intelligence*, 3(August):1–24.

Pushkar Mishra, Helen Yannakoudakis, and Eka- terina Shutova. 2019\.  [Tackling online abuse:](http://arxiv.org/abs/1908.06024) [A survey of automated abuse detection methods](http://arxiv.org/abs/1908.06024). *arXiv:1908.06024v2*, pages 1–17.

Robert C Nickerson, Upkar Varshney, and Jan Munter- mann. 2013\. [A method for taxonomy development](https://doi.org/10.1057/ejis.2012.26) [and its application in information systems](https://doi.org/10.1057/ejis.2012.26). *Euro- pean Journal of Information Systems*, 22(3):336– 359\.

Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020\. [Ad-](https://doi.org/10.18653/v1/2020.acl-main.441) [versarial NLI: A new benchmark for natural lan-](https://doi.org/10.18653/v1/2020.acl-main.441) [guage understanding](https://doi.org/10.18653/v1/2020.acl-main.441). In *Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics*, pages 4885–4901, Online. Association for Computational Linguistics.

Chikashi Nobata, Achint Thomas, Yashar Mehdad, Yi Chang, and Joel Tetreault. 2016\. [Abusive Lan-](https://doi.org/10.1145/2872427.2883062) [guage Detection in Online User Content](https://doi.org/10.1145/2872427.2883062). In *World Wide Web Conference*, pages 145–153.

Alexis Palmer, Christine Carr, Melissa Robinson, and Jordan Sanders. 2020\. COLD: Annotation scheme and evaluation data set for complex offensive lan- guage in English. *The Journal for Language Tech- nology and Computational Linguistics*, 34(1):1–28.

Endang Pamungkas, Valerio Basile, and Viviana Patti. 2020\. Misogyny Detection in Twitter: a Multilin- gual and Cross-Domain Study. *Information Process- ing and Management*, 57(6).

Georgios K. Pitsilis, Heri Ramampiaro, and Helge Langseth. 2018\. [Detecting Offensive Language in](http://arxiv.org/abs/1801.04433) [Tweets Using Deep Learning](http://arxiv.org/abs/1801.04433). *arXiv:1801.04433v1*, pages 1–17.

Fabio Poletto, Valerio Basile, Manuela Sanguinetti, Cristina Bosco, and Viviana Patti. 2020\. [Resources](https://doi.org/10.1007/s10579-020-09502-8) [and benchmark corpora for hate speech detection: a](https://doi.org/10.1007/s10579-020-09502-8) [systematic review](https://doi.org/10.1007/s10579-020-09502-8). *Language Resources and Evalu- ation*, pages 1–47.

Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020\. [Beyond accuracy: Be-](https://doi.org/10.18653/v1/2020.acl-main.442) [havioral testing of NLP models with CheckList](https://doi.org/10.18653/v1/2020.acl-main.442). In *Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics*, pages 4902– 4912, Online. Association for Computational Lin- guistics.

Paul Ro¨ttger, Bertram Vidgen, Dong Nguyen, Zeerak Waseem, Helen Margetts, and Janet Pierrehumbert. 2020\. [Hatecheck: Functional tests for hate speech](http://arxiv.org/abs/2012.15606) [detection models](http://arxiv.org/abs/2012.15606).

Joni Salminen, Maximilian Hopf, Shammur A. Chowd- hury, Soon gyo Jung, Hind Almerekhi, and Bernard J. Jansen. 2020\. [Developing an on-](https://doi.org/10.1186/s13673-019-0205-6) [line hate classifier for multiple social media plat-](https://doi.org/10.1186/s13673-019-0205-6) [forms](https://doi.org/10.1186/s13673-019-0205-6). *Human-centric Computing and Information Sciences*, 10(1):1–34.

Mattia Samory, Indira Sen, Julian Kohne, Fabian Flock, and Claudia Wagner. 2020\. [Unsex me here: Revisit-](http://arxiv.org/abs/2004.12764) [ing sexism detection using psychological scales and](http://arxiv.org/abs/2004.12764) [adversarial samples](http://arxiv.org/abs/2004.12764). *arXiv preprint:2004.12764v1*, pages 1–11.

Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, Noah A Smith, and Paul G Allen. 2019\. [The Risk of](http://www.figure-eight.com/) [Racial Bias in Hate Speech Detection](http://www.figure-eight.com/). In *Proceed- ings of the 57th Annual Meeting of the ACL*, pages 1668–1678.

Anna Schmidt and Michael Wiegand. 2017\. [A Sur-](https://doi.org/10.18653/v1/W17-1101) [vey on Hate Speech Detection using Natural Lan-](https://doi.org/10.18653/v1/W17-1101) [guage Processing](https://doi.org/10.18653/v1/W17-1101). In *Proceedings of the Fifth Inter- national Workshop on Natural Language Processing for Social Media*, pages 1–10. Association for Com- putational Linguistics.

Steve Durairaj Swamy, Anupam Jamatia, and Bjo¨rn Gamba¨ck. 2019\. [Studying generalisability across](https://doi.org/10.18653/v1/k19-1088) [abusive language detection datasets](https://doi.org/10.18653/v1/k19-1088). In *CoNLL 2019 \- 23rd Conference on Computational Natural Language Learning, Proceedings of the Conference*, pages 940–950.

Bertie Vidgen, Austin Botelho, David Broniatowski, Ella Guest, Matthew Hall, Helen Margetts, Rebekah Tromble, Zeerak Waseem, and Scott Hale. 2020\. [Detecting East Asian Prejudice on Social Media](http://arxiv.org/abs/2005.03909v1). *arXiv:2005.03909v1*, pages 1–12.

Bertie Vidgen and Leon Derczynski. 2020\. [Directions](http://arxiv.org/abs/2004.01670) [in Abusive Language Training Data: Garbage In,](http://arxiv.org/abs/2004.01670) [Garbage Out](http://arxiv.org/abs/2004.01670). *Plos ONE*, pages 1–26.

Bertie Vidgen, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott Hale, and Helen Margetts. 2019a. [Challenges and frontiers in abusive content detec-](https://doi.org/10.18653/v1/W19-3509) [tion](https://doi.org/10.18653/v1/W19-3509). In *Proceedings of the Third Workshop on Abu- sive Language Online (ACL)*, pages 80–93.

Bertie Vidgen, Helen Margetts, and Alex Harris. 2019b. [*How much online abuse is there? A systematic re-*](https://www.turing.ac.uk/people/programme-directors/helen-margetts) *[view of evidence for the UK](https://www.turing.ac.uk/people/programme-directors/helen-margetts)*. The Alan Turing Insti- tute, London.

Bertie Vidgen, Dong Nguyen, Helen Margetts, Patricia Rossini, and Rebekah Tromble. 2021\. [Introducing](https://www.aclweb.org/anthology/2021.naacl-main.182) [CAD: the contextual abuse dataset](https://www.aclweb.org/anthology/2021.naacl-main.182). In *Proceedings of the 2021 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies*, pages 2289–2303, Online. Association for Computational Linguistics.

Bertie Vidgen and Taha Yasseri. 2019\. [Detecting weak](https://arxiv.org/pdf/1812.10400.pdf) [and strong Islamophobic hate speech on social me-](https://arxiv.org/pdf/1812.10400.pdf) [dia](https://arxiv.org/pdf/1812.10400.pdf). *Journal of Information Technology & Politics*, 17(1):66–78.

William Warner and Julia Hirschberg. 2012\. [Detecting](http://dl.acm.org/citation.cfm?id=2390374.2390377) [hate speech on the world wide web](http://dl.acm.org/citation.cfm?id=2390374.2390377). In *Proceedings of the Second Workshop on Language in Social Me- dia*, pages 19–26.

Zeerak Waseem. 2016\. [Are you a racist or am I seeing](https://doi.org/10.18653/v1/W16-5618) [things? annotator influence on hate speech detection](https://doi.org/10.18653/v1/W16-5618) [on twitter](https://doi.org/10.18653/v1/W16-5618). In *Proceedings of the First Workshop on NLP and Computational Social Science*, pages 138– 142, Austin, Texas. Association for Computational Linguistics.

Zeerak Waseem, Thomas Davidson, Dana Warmsley, and Ingmar Weber. 2017\. [Understanding Abuse: A](https://doi.org/10.1080/17421770903114687) [Typology of Abusive Language Detection Subtasks](https://doi.org/10.1080/17421770903114687). In *Proceedings of the First Workshop on Abusive Language Online*, pages 78–84.

Zeerak Waseem and Dirk Hovy. 2016\. [Hateful Sym-](https://doi.org/10.18653/v1/n16-2013) [bols or Hateful People? Predictive Features for Hate](https://doi.org/10.18653/v1/n16-2013) [Speech Detection on Twitter](https://doi.org/10.18653/v1/n16-2013). In *NAACL-HLT*, pages 88–93.

Zeerak Waseem, James Thorne, and Joachim Bingel. 2018\. [Bridging the gaps: Multi task learning for](https://doi.org/10.1007/978-3-319-78583-7_3) [domain transfer of hate speech detection](https://doi.org/10.1007/978-3-319-78583-7_3). In Jennifer Golbeck, editor, *Online Harassment*, pages 29–55. Springer International Publishing, Cham.

Michael Wiegand, Josef Ruppenhofer, and Thomas Kleinbauer. 2019\. Detection of Abusive Language: the Problem of Biased Datasets. In *NAACL-HLT*, pages 602–608, Minneapolis. ACL.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R’emi Louf, Morgan Funtow- icz, and Jamie Brew. 2019\. Huggingface’s trans- formers: State-of-the-art natural language process- ing. *ArXiv*, abs/1910.03771.

Lucas Wright, Derek Ruths, Kelly P Dillon, Haji Mo- hammad Saleem, and Susan Benesch. 2017\. [Vec-](https://doi.org/10.18653/v1/W17-3009) [tors for counterspeech on Twitter](https://doi.org/10.18653/v1/W17-3009). In *Proceedings of the First Workshop on Abusive Language Online*, pages 57–62, Vancouver, BC, Canada. Association for Computational Linguistics.

Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017a. [Ex Machina: Personal Attacks Seen at Scale](https://doi.org/10.1039/C5CC05843K). In *Proceedings of the International World Wide Web Conference*, pages 1391–1399.

Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017b. [Ex Machina: Personal Attacks Seen at Scale](https://doi.org/10.1039/C5CC05843K). In *Proceedings of the International World Wide Web Conference*, pages 1391–1399.

Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019\. [Predicting the type and target of offensive](https://doi.org/10.18653/v1/n19-1144) [posts in social media](https://doi.org/10.18653/v1/n19-1144). In *Proceedings of NAACL HLT 2019*, volume 1, pages 1415–1420.

Marcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Hamdy Mubarak, Leon Derczynski, Zeses Pitenis, and C¸ ag˘rı C¸ o¨ltekin. 2020\. [SemEval-2020 Task 12: Multilingual Offen-](http://arxiv.org/abs/2006.07235) [sive Language Identification in Social Media (Offen-](http://arxiv.org/abs/2006.07235) [sEval 2020\)](http://arxiv.org/abs/2006.07235). *arXiv preprint*, pages 1–23.

1. # **List of identities** {#list-of-identities}

See Table [5](#category-of-identity-identity) for a list of identities focused on during data collection.

2. # **List of pivots in R2** {#list-of-pivots-in-r2}

## **Hate pivots**

**Convoluted statements** Statements which ex- press hate through a convoluted sentence structure,  
e.g. ‘I’ll make sure that X don’t exist anymore because that’s what I want to do and I dont care if nobody knows it”.

**Negation of neutrality in the second clause** Statements which initially appear to be neutral (or positive) about an identity, but then undermine it with a caveat, e.g. ‘some people say it’s not fair to call X dumb. . . but 9 times out of 10 it’s true\!’.

**Long phrases** Statements which are long, often involving multiple clauses and sentences. In such cases the hate may use more subtle forms of lan- guage and/or rely more heavily on tone, e.g. ‘Italy is a beautiful country and I love it so much, from its food to its fashion, from its history to its arts, but i am very worried about the current trends which re- mind me of the 1930s, So let’s import X to change up the food, fashion, history and arts?’

**Rhetorical questions** Statements which ask a rhetorical question in order to make a hateful point. Their goal is to express hate rather than to elicit an actual answer, e.g. ‘Aren’t all X penny pinching, after all?’

**Misspelling (swapped characters)** Words which are intentionally misspelt so that, although the meaning is clear to a human, the model misclassifiers it because the hate is ‘obfuscated’.[5](#5note-we-use-*-for-the-first-vowel-in-each-slur-to-minimize-use-of-harmful-language.)

e.g.  *n\*gga*:  ‘N\!gg@’, ‘n\!gga’, ‘n1gg4”, *f\*g*:

‘f@g’, ‘f4g’, ‘f@gz’, *k\*ke*: ‘k1ke’, ‘k1k3’.

**Misspelling (elongation)** Words which are mis- spelt so that they are far longer, with many repeated letters, e.g. ‘I think letting gaaaaaaaay people have rights is just soooooooo awful’.

**Misspelling (spaces)** Words which are misspelt so that there are spaces between letters. Mis- spelling of terms includes: ‘T R A N N Y S’, ‘J e w s’, ‘N i g g a’.

5Note we use \* for the first vowel in each slur to minimize use of harmful language.

**Synonym (hate expressed with infrequently used terms)** Statements which contain rarely used terms,, e.g. rather than saying ‘X are scum’ they use the phrase, ‘X are all sour misanthropes’. Equally, such statements might just direct unusual / less common forms of abuse at the targeted identity. Infrequently used terms to insult a group include: ‘Obnoxious’, ‘Heinous’, ‘Abhorrent’, ‘Unlawful’, ‘Dishonorable’.

**Terms with positive sentiment** Statements which contain positive phrases and terms but still express hate. In particular, phrases which use the term ’love’ are often identified as not hateful, e.g. ‘I just love it when all of the brown people leave my office so I dont have to look at them’.

**Random statement** Statements which are ’ran- dom’ and do not content any features which are likely to trick the model.

## **Not Hate pivots**

**Counter speech (by another identity)** State- ments which challenge, counter or dismiss hate, which are expressed by someone who does *not* identify with the group being attacked. This often involves referring to or quoting hateful content – which models may mistake for genuine hate.

**Counter speech (by target identity)** Statements which challenge, counter or dismiss hate, which are expressed by someone who is from the group being attacked.

**Counter speech (use of negation)** Statements which are neutral and state that a group does not have a (negative or neutral) attribute. This is pre- sented primarily as a contrast set to *Negated non- hate* (in the Hateful pivots), e.g. ‘Jews are not money lovers’.

**Not hateful use of a term that is also a referent for an identity (polysemy)** Statements that men- tion a referent for an identity (in a way that is not meant to refer to the identity). For instance, using ‘black’ as a colour, e.g. ‘the black cat down the road really ruins the neighbourhood’.

**Use of profanities** Statements which contain a profanity in a not hateful way. They are often used as an adjective or adverb to describe an emotion or to place emphasis on what is being said, e.g. ‘fucking hell today was a lot of bullshit’.

**Negativity against objects** Statements which at- tack, criticise or express negativity against inani- mate objects, such as sofa or a car, e.g. ‘this cup is totally rubbish’.

**Personal abuse (direct)** Statements which are aggressive, insulting or abusive against an individ- ual using a direct personal pronoun (i.e. ‘you’), e.g. ‘you are a complete joke and no-one respects you, loser’.

**Personal abuse (indirect)** Statements which are aggressive, insulting or abusive against an individ- ual who is not part of the conversation and as such is referred to with an indirect personal pronoun (i.e. ‘he’, ‘she’, ‘they’), e.g. ‘he is such a waste of space. I hope he dies’.

**Negativity against concepts** Statements which attack, criticise or express negativity against con- cepts and ideologies, such as political ideologies, economic ideas and philosophical ideals, e.g. ‘I’ve never trusted capitalism. It’s bullshit and it fucks society over’.

**Negativity against animals** Statements which attack, criticise or express negativity against an- imals, e.g. ‘dogs are just beasts, kick them if they annoy you’.

**Negativity against institutions** Statements which attack, criticise or express negativity against institutions; such as large organisations, governments and bodies, e.g. ‘the NHS is a badly run and pointless organisation which is the source of so much harm’.

**Negativity against others** Statements which at- tack, criticise or express negativity against some- thing that is NOT an identity – and the targets are not identified elsewhere in this typology, e.g. ‘the air round here is toxic, it smells like terrible’.

3. # **Development set performance** {#development-set-performance}

Table [6](#model-1) shows dev set performance numbers.

4. # **Model, Training, and Evaluation Details** {#model,-training,-and-evaluation-details}

The model architecture was the roberta-base model from Huggingface ([https://huggingface.co/](https://huggingface.co/)), with a sequence classification head. This model has approximately 125 million parameters. Training each model took no longer than approximately a

day, on average, with 8 GPUs on the FAIR clus- ter. All models were trained with a learning rate of 2e-5 with the default optimizer that Hugging- face’s sequence classification routine uses. Target model hyperparameter search was as follows: the R2 target was trained for 3 epochs on the R1 target training data, plus multiples of the round 1 data from {1, 5, 10, 20, 40, 100} (the best was 5). The R3 target was trained for 3 epochs on the R2 tar- get training data, plus multiples of the round 2 data from {1, 5, 10, 20, 40, 100} (the best was 100). The R4 target was trained on the R3 target training data for 4 epochs, plus multiples of the round 3 data from {1, 5, 10, 20, 40, 100, 200} (the best was 1); early stopping based on loss on the dev set (measured multiple times per epoch) was performed. The dev set we used for tuning target models was the latest dev set we had at each round. We did not perform hyperparameter search on the non-target models, with the exception of training 5 seeds of each and early stopping based on dev set loss throughout 4 training epochs. We recall that model performance typically did not vary by much more than 5% through our hyperparameter searches.

5. # **Data statement** {#data-statement}

Following [Bender and Friedman](#emily-m.-bender-and-batya-friedman.-2018.-data-statements-for-natural-language-processing:-toward-mitigating-system-bias-and-enabling-better-science.-transactions-of-the-association-for-computational-linguistics,-6:587–604.) ([2018](#emily-m.-bender-and-batya-friedman.-2018.-data-statements-for-natural-language-processing:-toward-mitigating-system-bias-and-enabling-better-science.-transactions-of-the-association-for-computational-linguistics,-6:587–604.)) we provide a data statement, which documents the process and provenance of the final dataset.

1. **CURATION RATIONALE** In order to study the potential of dynamically generated datasets for improving online hate detection, we used an online interface to generate a large-scale synthetic dataset of 40,000 entries, collected over 4 rounds, with a ‘model-in-the-loop’ design. Data was not sam- pled. Instead a team of trained annotators created synthetic content to enter into the interface.

2. **LANGUAGE VARIETY** All of the content is in English. We opted for English language due to the available annotation team, and resources and the project leaders’ expertise. The system that we developed could, in principle, be applied to other languages.

3. **SPEAKER DEMOGRAPHICS**  Due to the

synthetic nature of the dataset, the speakers are the same as the annotators.

4. ## **ANNOTATOR DEMOGRAPHICS** Anno-

tator demographics are reported in the paper, and

are reproduced here for fullness. Annotation guide- lines were created at the start of the project and then updated after each round. Annotations guide- lines will be publicly released with the dataset. We followed the guidance for protecting and monitor- ing annotator well-being provided by [Vidgen et al.](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.) ([2019a](#bertie-vidgen,-alex-harris,-dong-nguyen,-rebekah-tromble,-scott-hale,-and-helen-margetts.-2019a.-challenges-and-frontiers-in-abusive-content-detec--tion.-in-proceedings-of-the-third-workshop-on-abu--sive-language-online-\(acl\),-pages-80–93.)). 20 annotators were recruited. Ten were re- cruited to work for 12 weeks and ten were recruited for the final four weeks. Annotators completed dif- ferent amounts of work depending on their avail- ability, which is recorded in the dataset.

All annotators attended a project onboarding ses- sion, half day training session, at least one one-to- one session and a daily ’standup’ meeting when working. They were given a test assignment and guidelines to review before starting work and re- ceived feedback after each round. Annotators could ask the experts questions in real-time over a mes- saging platform.

Of the 20 annotators, 35% were male and 65%

were female. 65% were 18\-29 and 35% were 30\-

39. 10% were educated to high school level, 20% to undergraduate, 45% to taught masters and 25% to research degree (i.e. PhD). 70% were native En-

## **Category of identity	Identity** {#category-of-identity-identity}

Disability	People with disabilities

Gender	Gender minorities (e.g. non binary)

Gender	Women

Gender	Trans

Immigration status	Immigrants Immigration status	Foreigner Immigration status	Refugee Immigration status	Asylum seeker Race / Ethnicity	Black people

Race / Ethnicity	Indigenous

Race / Ethnicity	East Asians (e.g. China)

Race / Ethnicity	East Asians (e.g. Korea)

Race / Ethnicity	South East Asians  
(e.g. India)  
Race / Ethnicity	Pakistanis Aboriginal people  
glish speakers and 30% were non-native but fluent.

Respondents had a range of nationalities, includ-

Race / Ethnicity  
(e.g. Native Americans)  
ing British (60%), as well as Polish, Spanish and

Iraqi. Most annotators identified as ethnically white (70%), followed by Middle Eastern (20%) and two or more ethnicities (10%). Participants all used social media regularly, including 75% who used it more than once per day. All participants had seen other people targeted by online abuse before, and 55% had been targeted personally.

5. **SPEECH SITUATION** All data was created from 21st September 2020 until 14th January 2021. During the project annotators visited a range of online platforms, with adequate care and supervi- sion from the project leaders, to better understand online hate as it appears online.

6. **TEXT CHARACTERISTICS**  The composi-

tion of the dataset, including the distribution of the Primary label (Hate and Not) and the type (Deroga- tion, Animosity, Threatening, Support, Dehuman- ization and None Given) is described in the paper.

Race / Ethnicity	Mixed race Race / Ethnicity	Minority groups Race / Ethnicity	Arabs

Race / Ethnicity	Travellers (e.g. Roma)

Race / Ethnicity	People from Africa Religion or belief	Muslims  
Religion or belief	Jews

Sexual orientation	Gay Sexual orientation	Lesbian Sexual orientation	Bisexual National origin	Polish

Religion or belief	Hindus

Class	Working class

Race / Ethnicity	Hispanic (e.g. Latinx)

Intersectional	Black women

Intersectional	Black men

Intersectional	Indigenous women

Intersectional	Asian women

Intersectional	Muslim women

Table 5: List of high priority identities

| Model | R1 | R2 | R3 | R4 |
| :---- | :---: | :---: | :---: | :---: |
| M1 (R1 Target) | 41.4±0.91 | 61.06±0.43 | 58.18±0.69 | 55.46±0.63 |
| M2 (R2 Target) | **95.38**±**0.25** | 68.86±0.71 | 66.46±1.09 | 63.17±0.8 |
| M3 (R3 Target) | 94.55±0.65 | 85.04±0.63 | 76.77±0.57 | 74.4±0.9 |
| M4 (R4 Target) | 94.92±0.45 | **85.32**±**0.29** | **77.52**±**0.68** | **76.42**±**0.82** |
| M(R1) | **95.69**±**0.29** | 61.88±0.98 | 57.75±0.8 | 58.54±0.52 |
| M(R2) | 81.28±0.2 | **84.36**±**0.4** | 75.8±0.55 | **74.29**±**1.05** |
| M(R3) | 76.79±1.18 | 79.6±0.99 | 75.5±0.48 | 74.19±1.07 |
| M(R4) | 78.05±1.09 | 80.21±0.31 | **75.63**±**0.49** | 72.54±0.64 |
| M(R0+R1) | **93.92**±**0.3** | 69.43±1.58 | 65.48±0.48 | 63.99±0.74 |
| M(R0+R1+R2) | 93.13±0.24 | 82.82±0.8 | 73.66±0.75 | 72.28±0.84 |
| M(R0+R1+R2+R3) | 93.43±0.39 | 84.66±0.6 | 75.81±0.29 | **75.85**±**1.0** |
| M(R0+R1+R2+R3+R4) | 92.73±0.82 | **86.0**±**0.69** | **77.0**±**0.59** | 75.7±0.69 |

Table 6: Macro F1 with standard deviation over 5 training rounds, evaluated on each rounds’ dev set. Early- stopping is performed on the latest development set for each round where dev results are obtained at least once per epoch, out of four epochs.