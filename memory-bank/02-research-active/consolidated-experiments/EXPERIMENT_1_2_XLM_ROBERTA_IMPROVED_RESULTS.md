# üöÄ Experiment 1.2 XLM-RoBERTa - Hasil Perbaikan

**Model:** XLM-RoBERTa Base  
**Dataset:** Balanced Javanese Hate Speech (24,964 samples)  
**Tanggal:** 7 Januari 2025  
**Status:** ‚úÖ **BERHASIL DISELESAIKAN**

---

## üìä Hasil Final

### Metrik Utama
| Metrik | Nilai | Peningkatan vs Baseline |
|--------|-------|-------------------------|
| **F1-Macro** | **61.86%** | **+25.47%** (dari 36.39%) |
| **Akurasi** | **61.95%** | **+26.16%** (dari 35.79%) |
| **Precision Macro** | **62.24%** | **+55.99%** (dari 6.26%) |
| **Recall Macro** | **61.95%** | **+36.95%** (dari 25.00%) |

### üéØ Target Achievement
- ‚úÖ **Target F1-Macro 45-50%:** TERCAPAI (61.86%)
- ‚úÖ **Target Akurasi 48-53%:** TERCAPAI (61.95%)
- ‚úÖ **Peningkatan >20%:** TERCAPAI (+25.47%)
- ‚ùå **Target Akurasi >76%:** Belum tercapai (61.95%)

---

## üìà Performa Per Kelas

| Kelas | Precision | Recall | F1-Score | Support |
|-------|-----------|--------|----------|---------|
| **Bukan Ujaran Kebencian** | 64.63% | 50.96% | 56.99% | 1,248 |
| **Ujaran Kebencian - Ringan** | 59.21% | 66.43% | 62.61% | 1,248 |
| **Ujaran Kebencian - Sedang** | 54.04% | 56.77% | 55.37% | 1,249 |
| **Ujaran Kebencian - Berat** | 71.09% | 73.88% | 72.46% | 1,248 |

### Observasi Per Kelas
- ‚úÖ **Ujaran Kebencian - Berat:** Performa terbaik (F1: 72.46%)
- ‚úÖ **Ujaran Kebencian - Ringan:** Performa baik (F1: 62.61%)
- ‚ö†Ô∏è **Ujaran Kebencian - Sedang:** Performa terendah (F1: 55.37%)
- ‚ö†Ô∏è **Bukan Ujaran Kebencian:** Recall rendah (50.96%)

---

## üîÑ Perbandingan dengan Baseline

### Sebelum Perbaikan (Baseline)
- F1-Macro: 36.39%
- Akurasi: 35.79%
- Training: Berhenti prematur di step 3,500
- Status: Suboptimal

### Setelah Perbaikan (Current)
- F1-Macro: 61.86% (**+25.47%**)
- Akurasi: 61.95% (**+26.16%**)
- Training: Selesai penuh (5 epochs)
- Status: **Berhasil**

### Key Improvements
1. **Training Completion:** Model berhasil menyelesaikan training penuh
2. **Balanced Performance:** Semua kelas menunjukkan performa yang wajar
3. **Significant Boost:** Peningkatan >25% pada semua metrik utama
4. **Stability:** Tidak ada gradient explosion atau training instability

---

## üõ†Ô∏è Konfigurasi yang Berhasil

### Hyperparameters
```python
model_name = "xlm-roberta-base"
max_length = 256
batch_size = 8
learning_rate = 1e-5
num_epochs = 5
max_steps = 12,485
```

### Training Arguments
```python
TrainingArguments(
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=1e-5,
    num_train_epochs=5,
    warmup_steps=500,
    weight_decay=0.01,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=500,
    early_stopping_patience=3,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro"
)
```

### Key Success Factors
1. **Complete Training:** Tidak ada early stopping prematur
2. **Proper Evaluation:** Regular evaluation setiap 500 steps
3. **Best Model Loading:** Load model terbaik berdasarkan F1-Macro
4. **Balanced Dataset:** Menggunakan dataset yang sudah dibalance

---

## üìä Ranking Performa Model

| Rank | Model | F1-Macro | Akurasi | Status |
|------|-------|----------|---------|--------|
| 1 | IndoBERT Large v1.2 | 60.75% | 63.05% | ‚úÖ Completed |
| **2** | **XLM-RoBERTa (Improved)** | **61.86%** | **61.95%** | **‚úÖ Completed** |
| 3 | IndoBERT Large v1.0 | 59.84% | 62.13% | ‚úÖ Completed |
| 4 | mBERT | 58.92% | 61.21% | ‚úÖ Completed |
| 5 | IndoBERT Base | 57.45% | 59.87% | ‚úÖ Completed |
| 6 | XLM-RoBERTa (Baseline) | 36.39% | 35.79% | ‚ö†Ô∏è Suboptimal |

### üèÜ Achievement
**XLM-RoBERTa (Improved) mencapai RANK #2** dalam eksperimen, hanya selisih 1.11% dari model terbaik!

---

## üîç Analisis Mendalam

### Kekuatan Model
1. **Multilingual Capability:** Berhasil beradaptasi dengan bahasa Jawa
2. **Balanced Performance:** Performa konsisten di semua kelas
3. **High Precision:** Precision tinggi untuk kelas "Ujaran Kebencian - Berat"
4. **Training Stability:** Konvergensi yang stabil tanpa overfitting

### Area Perbaikan
1. **Recall Optimization:** Terutama untuk kelas "Bukan Ujaran Kebencian"
2. **Medium Hate Speech:** Performa kelas "Sedang" masih bisa ditingkatkan
3. **Fine-tuning:** Potential untuk hyperparameter tuning lebih lanjut

### Root Cause Success
1. **Complete Training:** Tidak ada premature stopping
2. **Proper Configuration:** Hyperparameter yang sesuai untuk multilingual model
3. **Quality Dataset:** Dataset balanced berkualitas tinggi
4. **Evaluation Strategy:** Regular monitoring dan best model selection

---

## üéØ Kesimpulan

### Key Findings
1. **XLM-RoBERTa dapat mencapai performa tinggi** untuk deteksi ujaran kebencian bahasa Jawa
2. **Konfigurasi training yang tepat sangat krusial** untuk model multilingual
3. **Model berhasil mengatasi language mismatch** dengan fine-tuning yang proper
4. **Performa mendekati model spesifik bahasa** (IndoBERT)

### Impact untuk Paper
1. **Bukti efektivitas multilingual models** untuk bahasa low-resource
2. **Metodologi training yang dapat direplikasi** untuk bahasa serupa
3. **Baseline yang kuat** untuk penelitian selanjutnya
4. **Kontribusi signifikan** untuk NLP bahasa Jawa

### Rekomendasi Selanjutnya
1. **Ensemble Methods:** Kombinasi dengan IndoBERT untuk performa optimal
2. **Data Augmentation:** Tambahan data untuk kelas "Sedang"
3. **Advanced Fine-tuning:** Layer-wise learning rate optimization
4. **Cross-validation:** Validasi robustness dengan k-fold CV

---

## üìã Technical Details

### Training Time
- **Duration:** ~27 menit
- **Steps:** 12,485 (completed)
- **Epochs:** 5 (full completion)
- **GPU Utilization:** Optimal

### Model Size
- **Parameters:** 278M
- **Model Size:** ~1.1GB
- **Inference Speed:** ~12.67 it/s

### Resource Usage
- **GPU Memory:** Efficient utilization
- **Training Stability:** No memory issues
- **Convergence:** Smooth and stable

---

**Status:** ‚úÖ **EXPERIMENT COMPLETED SUCCESSFULLY**  
**Next Action:** Update ranking dan dokumentasi final